
基于Bert的中文评价情感分析【耗时整理‖免费分享 】【耗时整理‖免费分享 】.mp4

OK，我们开始上课。呃，上节课呢，带着大家来了解一下我们的这个大模型平台啊hand face。然后我这边给大家提醒一下啊。就是大家目前学习的这个热情是很高涨的，这个是个好事情呃，但是呢，很多初学者。啊，尤其是我们做程序员的同学啊，很容易犯一个毛病啊，很容易犯个毛病，就是大家因为我我。

我我知道大家手头上是有前面的，这个应该有些同学是有前面的几期的，这个课程内容的是吧？嗯，我这样跟你们讲一下啊，就是你们报名来学一个东西啊。啊，我简单说一下，因为我觉得这个东西比较比较重要。就是大家来报一个培训班儿啊，培训班儿和你自己上网查资料，学习最大的区别是什么你们知道吗？它就是系统性。啊，简单花几分钟，

我说下这个问题啊，就各位学习啊。学习你们一定要注意，我们学一个东西，它得有一个什么得有个顺序逻辑。得按照我们的系统去学，千万千万千万不要在。我们讲haagen face的时候，我看很多同学跑去搞什么open web UI啊呃，搞那个什么lemma factory微调对吧？那些东西我们后面会讲到。啊，一定要有个循序渐进的这么一个过程，因为这个课程它安排是有逻辑性的。明白了吧，

课程的安排是有逻辑性的，你像我们这套课程为什么是一开始讲的是大模型微调？为什么一开始大模型微调？因为这个玩意儿是基础。大模型我我微调完了之后才是rug。啊，才是Rock，所以说各位不要去着急啊，不要着急嗯，不然的话当我们讲到大模型微调的时候。你其实学起来你会感觉很无趣的，感觉自己好像前面已经搞过了，对吧？很多实验感觉已经做过了，但是呢，

又感觉没学透。好，这个要注意啊，要注意就是不要着急啊，不要着急，一定要把当前该学的东西要学好。然后呃，这简单说一下，其实大模型的这个应用课程。我们不会涉及到太多需要编程的东西。呃，本质上来讲，整套课程最复杂的东西其实是我们前面这节课，就是当我们给大家讲这个transformer结构的时候。啊，

因为这个过程是涉及到一部分原理的。呃，注意这个过程是涉及到一部分大模型原理的东西的，所以说这块其实是很很关键的，反而越往后面走哈，这个课程你会感觉它实操性，上属性的难度会小一些。啊，如果说要说难度的话呢？难度在在于装环境上面啊，难度在于装环境上面，对于我们实操来讲的话，难度其实会慢慢的降低。最开始实施上是最难的。所以说一定要把前面的这个基础要打扎实哈。

好，那么其他的就不多说了。就是希望大家就是学习这个课程，不要着急啊，那看一下我们今晚的内容，上节课的话呢，我们带着各位一起来学习了一下这个haagen face里面的。基础的一个体验吧，对吧？重点是学了两个部分，第一个部分就是怎么使用haag on face的，这个在线的API啊来调用我们的一些模型。但这个东西不具备实用性啊啊，只是给大家就是做个介绍，真正有实用性的东西，

也就是现在最具价值的还是在于私有化部署。所以说上节课一个本地本地的这个拉取模型啊，这一块儿是非常关键的，就怎么把我们哈登face平台上面的模型拉取到本地？啊拉取到本地，那么我们后面所涉及到所有大模型哈，跟上节课讲的这种本地拉取的方法是一样的。唯一的区别就在于呢，我们后面用的这个模型比较大，我们首推其实是国内的，国内的这个hagen face啊，国国内的这个modo scope。hang face的话呢，我们一般不优先考虑啊，因为它的网络下载比较慢一些，

但是方法是一样的啊，所以说把这个本地获取模型的这个东西要。注意一下啊。好，那么我们今天的话呢，就基于上节课获取的本地的这个bert模型来实现我们第一个应用啊，先做一个入门的一个微调。啊，入门的一个微调好，那么我们今天要实现的一个效果呢？就是嗯，实现一个呃，评论评论评论的一个情绪分析啊，评论的情绪分析就是我判断人家这个评价是。正向的还是逆向的，

对吧？那么要完成这么一个项目案例的话呢？我们需要涉及到这么几个知识点，第一个知识点。这个东西非常关键啊。我们今晚学的这个第一个知识点非常关键，它会贯穿你整个nlp的整套课程。什么东西呢？第一个知识点，我们就给大家介绍一下，就是模型是怎么来处理文本的？啊，模型是怎么处理文本的？实际上对于自然语义而言，最难的一个地方就在于我们怎么把这个字符？

怎么把这个字符给它变成什么模型？可输入的东西，那模型可输入的东西好啊，这是我们第一个重点。呃，然后的话呢，就是一个概念啊，就是我们现在这个阶段学的是弹幕性微调，对吧？那么什么是微调啊？给大家简单做一个介绍。我们今晚的话呢，用的是微调中的第一种方法，叫做增量微调，那么增量微调的话呢，

就涉及到这个下游任务的一个设计。啊，包括呃，大家在这里面要学习的一个思想，就是说我们怎么去设计一个下游任务，这个下游任务它一般是跟我们的业务逻辑挂钩的啊，就跟我们的需求是挂钩的。我们得根据需求来进行设计啊，好然后。啊，有了这个呃字符编码，我们知道模型微调的概念啊，然后我们设计一个下游任务之后的话呢。我们就可以自定义一个模型来开始训练了啊，开始训练我们的这个任务了啊，

我们今晚的这个实实验呢，就是中文评价的一个分析模型啊。OK，好。嗯，那么我们第一件事啊，第一件事先给大家介绍一下这个字符编码集啊，字符编码集好，那么这就直接。开始我们的实操啊。哎，因为今晚的实操会比较多一些。OK，我们新建一个项目。呃，

今天是这个demo零二啊？好，那么我这儿也说一下，就是因为我看到有同学在群里面在问关于这个环境的问题，是这样的。我们前面这几节课啊。不需要你们用的gpu，你有gpu最好，没有gpu的话CPU也可以上，不影响，明白了吧啊，不影响。所以说CPU的话就慢一点点啊，但是跑是可以跑的啊，没问题的，

不用还有一点就是大家不要过多的去纠结这个环境的问题，就只要你的环境能跑通，你就你就先跑着。因为环境这个东西啊。它会伴随你开发一生，就是你只要是干这一行，你在环境上面一定会遇到问题，包括对于我现在来讲哈。经常也会出现一些环境的问题，当然出了问题之后，我们慢慢慢的要找到它的解决方法啊，这个东西得慢慢积累，急不来。对于大家来现在来讲的话呢，我的建议就是你一开始不要过多的去纠结环境啊，

能跑代码就行啊，好这儿让它加载一下。应该差不多了。嗯，我们用的模型的话呢，还是上节课的那个bert啊，这节课我们就不下载了啊，我就直接把那个模型拷过来。把这个模型就直接拷过来，下来我们上节课已经说过了啊，我们打开给大家看一下。嗯，然后这个拍上拷贝东西的时候注意一下，你不要直接粘贴到那个工具栏里面啊，你把它文件夹打开，

粘贴到文件夹里面要快一些。如果是通过那个拍摄面去拷贝的话，速度会特别慢啊。好，这是我们上节课下载的那个bird based Chinese啊。打开看一下。对吧，跟我们上节课下的那个结构是一样的，一般来讲就是说这个东西我上节课强调过啊，我们下载下来的大模型里面，它必须得包含这么几个东西，一个是config配置文件。另外一个就是模型的权重文件，然后就是这个talking letter啊，就是我们的这个编码器啊，

字符编码器呃vocab的话呢，现在有些模型有啊。有些模型是没有的，实际上大多数模型是没有这个东西的，现在它把这个东西给隐隐隐藏成一个二进制文件了啊，一般不以TXT的形式展现，为什么呢？我们待会给大家讲啊。OK，那我们先说一下这个呃编码集的问题呃字符编码的问题。啊，字符编码的问题OK，我们来看一下第一个案例。咳咳咳。好test。

我们叫talking text啊。talking test.啊，我们先来看一下这个token。黑屏吗？哎，我看到有同学说这个。画面是黑的。呃，回复一下，我看一下是我的问题还是你们的问题？正常的是吧？那应该就是我们其他同学的问题啊，这个你们切换一下网络，或者重新进一下直播间啊。

好，我们就开始啊好，我先说一下这个东西，我们在这一小节里面重点是给大家介绍什么东西呢？啊，说一下。本小节。核心。介绍啊，我们这一节小节里面给大家介绍一下，就是呃AI模型AI模型。是如何？处理。字符，字符数据。

这个东西其实很难，我说一下啊，这个东西很难。呃，我们今天的话呢，重点是给你们讲一下这个呃它。呃，从应用层面上面，然后原理上面我们设计的不会特别的深，因为这个原理的话有点复杂啊，我会给大家适可而适可而止的做一个介绍啊。我简单讲一下AI这个东西，我在上节课提到过了，我们分为四大方向呃，其实入门的是图像视觉。

然后图像识别往上一点难一点的就是语音。最简单的实际上是视觉，然后再往后就是音频，再往后深度学习，里面最复杂的就是这个自然语义nfp nfp为什么难？为什么难哈？它难就难在啊，注意难在什么地方呢？就是我们现在所用的这些文本。你怎么把它变成模型？可输入的数据，那么这里面我们给大家简单讲一下啊，这个现在的所有的这个AI模型听清楚啊，所有的AI模型它的本质都是个什么东西，都是个矩阵。

把这一点要记住啊。所有的深度学习模型，它的本质都是个矩阵。都是个矩阵，那么这个矩阵处理的数据就是我们所谓的可输入的数据，这个数据x它本质上也应该是个什么？是个矩阵或者是个高位矩阵就张量，我们一般在深度学习里面用的是高位矩阵叫张量。啊，说透彻一点，它就是一堆数字。啊，这个输入的数据必须转成一堆数字。那么，对于这个自然语义领域的问题来讲的话呢，

它最难的地方在哪呢？就是我怎么把一句话给它变成一串数字呢？我应该或者我说直白一点，我应该用怎么样的一串数字来代表这一段文本呢？这是个很复杂的问题。啊，因为本质上来讲，我们的文本跟数字是没有直接对应关系的，听清楚啊，文本跟数字是没有直接对应关系的。我前面所说到的这个音频视觉为什么简单？因为拿视视觉来讲，我们一张图像。图像本质是由像素构成的，而像素就是什么？

数字像素是零到255的数字呃，大家这个我我我应该觉得大部分同学应该都知道，对吧？图像它本来就是数字，所以你不需要做转换图像就可以直接交给这个什么AI模型做处理音频一样的道理，音频这个东西的话呢，它是通过。我们的这个什么呢？采样的形式啊，采样的形式主要采的是这个，这个什么声音的，这个这个幅值啊，根据我们固定的采样率，采纳的幅值，所以它本质上也是一串数字。

所以对于整个AI行业来讲的话呢，其实比较简单的是视觉和这个音频啊，音频这两个东西入门起来比较比较好理解。最难的就是自然语义啊，最难的就是自然语义，因为我们这里面其实一开始就有一个很难的问题，就是怎么把这个什么呢？怎么把一串呃字符变成一串数字呢？这个东西你不能随便变，那不能随便随便做处理，它得有一定的依据。啊得有一定的依据好，那么我们今晚的第一个小节就是带着大家一起来体会一下，就是我们现在的这个transformer模型，它是怎么把这个？

字符变成AI模型，可处理的这个什么数值的对吧？好，那么在这个自然语义处理数据的过程中啊，它有一个非常重要的工具。就是我们的这个分词工具，分词器叫token izer。那么，它是包含在transformer这个包里面的。啊，我们导一下啊transformer in port好，把我们的这个talking net导进来，我们现在用的是bert模型啊，我们就导入这个bert。这个bert bert token neither啊token neither好，

我们来看一下啊，那么现在大模型里面啊，所有大模型里面它的做法。听清楚啊，所有大模型里面所有的自然语大模型里面做法就跟我们今晚讲的东西是一样的，我们第一步第一步要处理文本数据，我们得借助于这个字典和分词器啊。所以第一步我们加载。加载字典。和分词器啊，加载字典和分词器talking。talking就等于我们的bert talking，neither点from train这个我们上节课讲过了啊，好直接把它加进来。这注意一点，

就是说我们在这导入这个大模型的文件夹的时候。因为对于这个bart模型来说，它已经比较早了啊，比较比较久远的大模型了。那么，它的根目录一定要导在包含config的这个文件夹下面，而且呢，我们必须要拷贝它的这个绝对路径，要带有盘符啊，不带盘符不行。它会联网去获取。好，这个分词器我们就加进来了，把它拉过来。分支器我们就加进来了呃，

加进来之后的话，我们可以简单查看一下这个token啊，把它输出看一下。还有一点，我说一下，因为呃p这个编程语言哈，它有一个特点。它有个什么特点啊，你们记住一句话，在passion中，在passion中，万物皆是对象啊，万物皆是对象。那么，既然是对象，

这个东西就可以拿来做print。所以我们用pation做开发的时候，有一个特点，如果我拿了一个对象，我想简单看一下它是个什么东西，对吧？最简单的方法就是你直接输出它。任何东西都可以print啊，任何东西都可以print。啊，这个第一次加载稍微慢一点点啊。看一下呃，它第一次加模型要慢一点，因为我这边环境刚刚是导进来的。我们来加一下，

加载完了之后，这print之后，你其实可以可以看到它会输出这个token的一些什么属性啊，会输出token的一些属性。呃，一般来讲token里面有这么几个比较重要的属性，待会我们就可以看到哈，第一个就是这个这个分词器啊，这个分词器。好哦，我们可以看到你看啊，这出来了啊，这出来了，第一个特点就是它会输出这个分词器，里面能够所识别的字符有多少个？

在哪里呢？我们看一下啊。这里面有个什么有个5 cup size。啊vocab site就是我们当前这个talking里面所包含的这个什么字符的数量有二幺幺二八啊，有二幺幺二八。好，那么这个是个什么意思呢？给大家详细做个说明我，我说一下，就是现在我们的这个什么呢？嗯，自然语言模型啊，它在处理字符的时候，它怎么来处理呢？它不是凭空来做的，

其实它是借鉴了计算机去存储字符的一个方法。啊，计算机去存储字符的一个方法，我打个比方，你比如说我们现在这个WINDOWS系统Windows系统，它存放这个字符啊，存放这个字符，比如说我们存放这个存放这个中文的这个字符，它怎么存放的呢？计算机里面它只能存二进制的数据，它存的就是零一。它存的就是零一，那这个零一我怎么给它转换成是这个字符呢？这个零一哈这个零一我们这简单简单说一下。这个零这个东西是二进制的，

对吧？二进制的那么这个二进制的话呢？我们转换它的时候，它有。至少现在有两种转换方法，第一种这个二进制数，我是不是可以转换成一个十进制数啊？我可以转换成一个十进制数是吧？如果转换成11个十进制十进制数的话呢？那那它就表达一个阿拉伯数字啊，这个零的话就是一。其次的话呢，它可以表达一个字符。啊，它可以表达一个字符。

假如说这个字符放到我们中文上面，就是一个什么，就是一个大啊，它可以表达一个字符。那么，它什么时候表达数字？什么时候表达字符呢？这就是由我们所学的所有的编程语言里面，它有个它有个第一个基础性质叫做数据类型。数据类型，在干一件什么事？其实，数据类型简单来讲就是告诉计算机这个零一。此时此刻，到底表达的是个数值还是个字符？

啊，还是个字符。OK好，那么问题来了，当它要表达一个字符的时候，零一为什么对应到这个大上面去了呢？它得借助于一张表，这个表叫做什么编码表？啊，这个应该都应该都知道是吧？学过计算机的同学应该都知道啊，这个鼠标不好写字啊，你们呃，注意听我的这个含义就行了啊。这个表就叫编码表，

所以你像我们计计算机里面，它用来对应字符和这个二进制之间这个关系就是通过一张编码表来实现的。OK，那么我们现在的这个自然语言模型里面哈，它在识别这个文字的时候也是借鉴了这种思想，它是基于一个什么东西，它是基于一个字典。啊，注意听它把就是这个模型，把所需要对应识别的所有字符是存放在一个字典中的。明白了吧啊，是存放在一个字典中的，然后这个vocab size里面就是这个字典里面的字符数量。啊，就是这个字典里面的字符的数量好，

那么我们现在所看到这个bert模型的一个字典，人家是给你暴露暴露出来的啊，它是一个文本，它是一个这个字符文件。就是这个我cup点text看到没有？那么，它里面一共的长度是二幺幺二八，就是说我这个模型目前能够识别出来的所有的字符。都在这个文件里面啊，就是这二幺幺二八个字，我们拉到最下面看一下，看到没有啊，就是二幺幺二八个字符。啊二幺幺二八个字符。好啊。

哎，所以说呃，搞清楚第一个思想就是所有的自然语言模型。听清楚啊，它识别的这个字符是有限的。除以啊，所有的模型，包括现在的很火的deeps。一样的啊chart cpt一样的，它识别的字符是有限的。就是说你这个模型里面的这个vocab放了多少个字符，你的模型就能够识别出来多少个字？OK好，那么我上节课也简单提到过，那万一就是说人家现在输入的这段文本很特殊，

特殊在什么地方呢？就是我里面有些字符是不包含你的，不不存在于你的这个二幺幺二八个范围内的，那会怎么办？那就用到它的第二个东西，叫做special talkings。看到没有啊，这是我们刚刚看到的这个talking里面，它就有一个比较特殊的东西，叫special talkings，就这个special talkings就专门用来处理一些比较特殊的情况。比如说它第一个第一个叫unk talking就是unknown啊，unknown就是你当前的这个字符，如果。我不存在于我的这个vocab里面，

那么我们就没法对应了，对吧？没法对应不就报错了吗？那么怎么来避免这个错误呢？我就用unk来代表。啊，你的这个不存在的这个字符，只要是不存在于我二幺二二八这个字符，我们会全部编码成unk啊，就这个意思啊。啊，全部编码成unk啊。好，那么接下来重点来到主题。不存在的字符用uak表示，

那如果存在的字符呢？打个比方。打个比方。假如说我们现在有一句话啊，我这用这个记事本说一下。token编码。token的字符编码啊，比如说我们现在有一段文本。我们要让这个。呃token进行编码，假如说的文本是这样叫做白日依山尽。就这么一句话，对吧？这个逗号啊，这个逗号好，

那么这个白日依山尽，我要让这个token进行编码，它咋编码呢？它咋编码呢？注意看啊。它就会去查找这个vocab里面。啊，我们来找一下啊，这个白字。啊，这个白字啊，它就会查找这个vacab里面字典里面这个白字啊，这个白的位置这儿说一下啊，注意看。其实我们现在这个用的这个编码表，

你搜一个字，它会存在两个结果。呃，我提前给你们解答一下这个疑惑啊，避免有有同下去，有同学去看了之后就有点蒙圈了，你看它会存在两个结果，一个是就是个白。一个白字儿的前面会加两个井号啊，会加两个井号，这两个东西有没有什么区别呢？这两个东西有区别，它主要区别于就是说我们对于这个词进行拆分的时候。拆分的方式不太一样，你们记住结果就行了，

前面没有任何东西的这个白字呢，它就是单个的字符。就是单个的字符就什么意思呢？中文这个东西啊，分词的方式有两种。一种是不带上下文关系的，另外一种是带有上下文关系的。而且我们现在的这个中文啊，一般在进行编码的时候。啊，这儿稍微讲一下，因为我们讲讲编码这个东西讲透啊，这个中文编码的时候，它涉及到一个问题，它第一步得把句子转成什么？

把句子转成词。把句子转成词，中文在分词的时候有两种方法，常用的主流方法有两种，第一种就是我单个的文字。变成一个词。第二种就是我们的词组。啊，我们的词组如果说以单个的文字进行进行这个分词的话呢，你像白日依山尽逗号，它就是六个词。啊六个词，如果以词组的形式来分分这个分词的话呢，那白日啊就是一个词啊，衣衫。

近啊，但这个这个古诗词可能我分的不是特别标准啊，就你们把这个含义理解了就行了啊，但但是现在的这个大多数的注意听啊，现在大多数的这个基础模型啊，就是我们用的这个预训练的模型。你会发现呢？它们采用的中文的分词用的是单个的字符来分的词，为什么用单个的字符来分词？单个字符分词有个优势。有个优势，它的扩展性强。优势在于扩展性下。啊，就是扩展性下什么意思啊？

中文这个玩意儿有个特殊的地方在于它是有些词是多义的。有些词是多义的。啊，你比如说我们这个单。单的话呢，我有很多种组合方法，我我可以组词为单人，对吧？单个啊，单个都可以。那么，如果说我们分词的时候，以词组的分了，那么你这个单个啊，就是绑在一起了，

你的扩展性就不强了。明白了吧啊，就是这个意思哈，所以很多的这个预训练模型里面哈，现在这个你像这个bert base里面，它是单个的字符啊，单个的字符不带上下文关系的。那么，这种做法，它的训练的难度会增高，但是模型训练好之后，扩展性就会很强，就是我基本适用于所有的这个中文语料啊，就这个意思，大家把这个含义理解了就行了。

那么下面这个带有两个井号的啊，前面带有两个井号的，它是包含了上下文关系的。它是包含一定的上下文关系的，就这个白字儿啊，人家是前面有一个词或者后面有个有一个词，明白了吧啊，前面有个词，后面有个词啊，这是它用的两种不同的编码形式。注意一下嗯，一般来讲的话呢，我们你像我们现在用的这个默认的这个词组形式啊，其实都是单个的字符啊，单个的字符，

所以严格意义上来来说这个二幺幺二八。二幺幺二八，你得给它除个二啊。它真正能够识别的字符，应该要拿这个二幺幺二八去除以个二。好哎，把这个字符注意一下，那么打个比方，我们继续我们的重点，你像这个白日依山尽，接下来我要对这句话编码编码的本质是什么？你得把就是把文字。把文字我说一下啊，把字符。转数字。

因为对于AI的这个模型来说的话，它只能够处理数字，我们得想办法把这个字符转成数字，才能够输到模型中，那这个白日先生怎么转成数字的呢？很简单哈，你看啊，拿这个五开五来讲嗯，因为我们这里面的这个开头的索引是从哦，这索引是从一开始的是吧？从一开始的那就我们搜一下吧啊。这个白。百啊百，我们找第一个这个百，这个就是四六三六。

四六三六就是四，注意啊，这个字典里面字符有个特点，它是唯一的。它是唯一的，而且它是有顺序的。那么，这个白字在字典中所存放的索引位置是四六三六四六三六四六三六，我们一般索引是从零开始的，所以这里面就四六三五。四六三五就代表是白。啊四六三五九代表的是百，然后这个日我们这简单搜一下啊。呃，日。

是三幺九零，那就是三幺八九啊。三幺八九啊，因为正常的索引是从零开始的啊，它这个上面是一开始的，所以我们要给它减减一下啊，减一下。好一。一这个是八九九，那就是八九八。啊，在这啊。八九八啊，八九八然后。山呃二二五五。

二二五五。进啊，我们先把它写出来啊，待会我们会验证这个编码到底跟我们讲的是不是一样的？那就是二二六啊，二二二六。二二二六还有个逗号，还有个逗号，那么逗号我们来搜一下啊。好，这儿我们输个中文的逗号，中文逗号是八零二五。啊，逗号也也也会有这个上下文啊，八零二五我们比如说八零二四啊，

八零二四。所以说我告诉各位，首先这个talking的编码怎么来做的？其实它的思思想非常简单，非常简单。就是你输入一段文本。然后呢？talking要做的这个编码的方法呢？就是去寻找你这段文本所对应，在它的这个字典中的什么下标？明白了吧，这样就把它变成了一串数字啊，但是注意啊，这个这串数字不是词向量哈。它可不是词向量听清楚啊，

这个玩意儿不是词向量啊，它只是字典中的一个索引。啊，它并并不是我们所谓前面刚刚所讲的那个词，向量啊词向量OK好。这把这个先记住啊呃，这是它它的一个基本的编码方法，我们来接下来我们用这个代码来验证一下。好，拿到这个token之后。啊，它里面的内容刚刚说过了啊，就这么几个东西，对吧？特殊字符看到没有？

然后它下面下面这放的是这六个特殊字符的位置。PID是在零，unk是在100啊cls是在101，但注意啊，它这里面是从什么索引是从零开始的啊？我看不到索引是从一开始的。就是这个地方，我们不是说我看不索引，就是我们现在看到这个上面这个索引是从一开始的啊，然后这个是零开始的，正常正常的索引都是从零开始的，要注意一下，所以我刚刚换那个索引时候是减了个一的啊，减了个一的。OK，

这是talking啊，talking了解之后，接下来我们来用一下，看一下我们刚刚给大家讲了一个编码，到底是不是那么一回事啊？好，我们来验证一下，那怎么来验证呢？很简单，我们接下来就构建一段什么呢文本？啊，构建文本。呃，准备。准备要编码的。

文本数据。呃，准备要编码的文本数据。sentence啊，sentence好，我们准备呃两个号吧啊，白日依。依山尽。啊白日依山尽逗号对吧？这个我们来验证一下，然后我再准备一段长一点的，因为这个长短我们待会儿。有用啊，再准备一段长一点的。我从那个评价数据里面拿一句话出来啊，

拿一句话出来，稍微长一点点。OK好，我们这儿就拿两句话，假如说我现在编码的就这两句话好，那么接下来我们来编码编码的这个方法呃比较多哈，我这儿就直接给大家。讲我们最实用的，我们直接用这个批量。批量编码句子。啊区量批量编码，我们的语句这个out就等于接下来用用，我们刚刚加的这个talking来进行编码分词加编码啊talking点。它的批量编码叫做betch decode呃encode啊，betch encode plus啊，

批次编码啊，加强版。就是它可以一次性的编码，多句话好，那么编码的这个函数里面的话呢，我们所需要传入的参数是这么几个东西，第一个参数。第一个参数就是你要编码的语句。啊，把我们要编码的这个语句啊，语句给它拿进来。好，那么这儿语句我们是两句话对吧？sentence 0和sentence 1啊，给它加进来sentence 0。

还有这个sentence 1啊，给它加进来OK好，这是传入我们要编码的这个什么语句啊，传入我们要编码的语句。然后第二个参数add special talkings就是否要添加这个特殊字符，这个一般都要添加的啊，一般都要添加的。啊好，那么第三个参数，第三个参数叫做truncation啊，truncation什么意思？截断的意思？截断的意思，一般我们都要把这个闯给设置置为true，如果你不设置为true，

其实我们上节课在用那个。gbt gbt的模型啊，默认没有加这个传给人处的时候，你会发现它有个警告，对吧？那么这个参数是个什么意思呢？我说一下啊。加个注释。就是当我们的句子。呃，长度。大于max less。呃大于max lens时。啊，就将这个语句什么截断？

截断啊。大于max lens是阶段，那么这个max lens是我们可以自己设计的。注意，这个max lens我们可以自己设计的。max lens等于比如说我这个max lens，我就给到这个一二三四五六就给到六。你大于六的话就截断啊，大于六的话就截断OK？啊，当然我说呀，这个max less它有个上限。这个玩意儿有个上限啊。这东西是有上限的，上限在哪儿呢？

上限就是我们刚刚这所看到这个什么呢？这个长度看到没有啊？它的上限是model。我这加个注释啊。上限。是上限是。model max lence啊，就是你这给max lence说你不能超过512，超过512就报错了啊，超过512就报错了啊。OK，好，这是max lens啊，然后还有一个参数叫padding。那你有长的就有什么就有短的。

啊padding我们一般也设置为max s，那么这个时候是个什么意思呢？就是当你的长度不够的时候啊，那么就一律补零。一律补这个零啊，到什么max？就长了就截，断短了就干什么补齐，为什么要这样操作呢？原因很简单。我刚刚说过了AI模型是个矩阵。是个矩阵。它本质上做的是矩阵运算，那么你的输入数据也得是个矩阵，明白了吧？

你的输入数据也得是个矩阵，比如说我一句话代代表代表一个向量。那我多句话，是不是就代表是多行向量了，对吧？那多行向量就构成了一个矩阵，矩阵有个要求。每个向量的长度得一致，所以说我们这个文本哈最麻烦的地方在于有些语句短一些，有些语句长一些，你得干嘛？你得对齐它，就这个意思啊，不对齐的话，它没法做操作。

理解没有啊，你得对齐它，所以我们这儿相当于说通过这两个参数定义了一种对齐的标准，标准是什么？标准就是六。啊，标准就是六这个长度就为六，你不够六长，我用零去补齐它啊，你超出六了就不要了。明白了吗啊，就这个意思啊，好那么这个max lens给的时候有没有讲究呢？有讲究有讲究，我们一般情况下max lens得给你得根据你的这个文本。

就你的这个数据来进行设计啊，这个我们待会项目时候再给你们实际的去操作啊，你们先把这个含义记住啊。OK，就是max dense好，那么后面还有几个参数啊？还有几个比较重要的参数，这是属于基础的设置。接下来的话呢，我们可以设置一下它编码中的返回类型return tensors。啊return tensors我们用一个null啊，就是它的默认值，我说一下这块儿的话就代表什么意思呢？我们编码之后返回的这个数值。它为什么类型啊？

它为什么类型？这个return tensors可取的值有这么几种，有TF这个TF代表什么呢？代表。tensor flow的张量。啊，代表tensor flow的增量PT的话，代表是paddle的增量non pai就是non pai的这个array类型。如果给long就是默认，默认就是list啊，默认就是list啊，我们现在先不看那么复杂哈，重点看一下它的这个这个结果，所以我就默认用这个none来表示啊。啊，

用这个long来表示啊。好，那么接下来有接下来的重点来了啊，接下来重点来了，接下来有三个返回值，我们需要返回return。第一个叫attention mask。啊好，然后第二个重要的返回值叫做talking type IDS。呃，talking type IDS，然后第三个叫做special talking mask。special talking mask.设置为true。这三个东西是什么东西呢？

这三个东西是什么东西呢？我说一下啊，这是我们编码之后，编码之后需要给到现在的这个transformer模型的输入。我们现在模transform transformer模型里面，它就需要用到这么三个东西，具体它是个什么玩意儿？不着急哈，我待会打印出来之后再给你们去看，你们就理解了，现在讲的话你理解不到啊，先不着急。呃，题外话啊，就是这后面这些值可以不给，

为什么可以不给呢？我们点进去看一下，看一下这个函数。因为像我刚刚这所提到的，你看啊，这个talking type IDS，然后attention mask还有这个special talking mask。它它这个都有默认值。啊，都有默认值，只不过这个special talking mask我们一般认为FALSE啊，因为这个东西一般的任务用不到啊，一般任务用不到啊，我们先把它打打出来给你们看一下。OK啊，

那么最后的话，我们还要需要返回一个这个长度啊return length。啊，设置为true这个是返回。我们的这个lens长度啊。这个l长度就是你编码之后的序列长度啊，返回。序列长度。就你你编码之后的这个文本是多长啊？是多长好，一般我们编码里面需要设计的这个参数就这些。啊，就这些，你需要控制的，就这些参数啊，

好那么有了这个talking之后啊，这个这已经编码完了，对吧？编码了编码了之后，我们接下来看一眼。那我们来看一下它这个返回值啊，看这个返回值，这个返回值怎么来看呢？这个返回值怎么来看呢？我们说我们直接来print一下这个out给你们看一下啊，你直接print这个out不好看啊。我们先print print out给你们看一下。直接print out的话呢，它看起来没有那么的清晰。啊，

我们就先不用这个out了，这先让它跑，跑完之后给你们看一下啊，你看啊，你直接print的话，它是堆在一起的，不太好看啊，不太好看，那我们这想个办法。这个东西输出的结果是一个什么东西呢？是一个字典。看到没有，它是一个字典哦不啊，它是个这个p里面这个字典对象，然后我们这就去遍历它的这个间值段啊，

间值段我们按照这个间值段来进行输出。fork vin out.点items。好，我们这来输出一下这个k。中间加一个这个冒号隔开啊。呃kv啊，我们来看一下这几个结果啊。尤其是这个第一句话，注意看一下啊。好，先看第一个结果，这个返回值里面它有一个东西叫做input IDS，这个input IDS是个什么东西呢？input IDS就是编码之后的词，

就是talking编码之后的词啊，我把注释加在这里。input IDS就是talking编码之后的词，那么我们刚刚自己编了一个码，我们来看一下人家这个编码跟我们编的码是不是一样的，对吧？我们刚刚给大家数了一下，你看啊，我们第一个编码是。四六三五它这多了个101，那个101是什么？你先不管啊，你看第一个白字是不是就四六三五看到没有？是不是对应起来了？日三幺八九三幺八九下来是八九八八九八啊，

然后是二二五五二二五五。啊，二五五。哎，我有个逗号的嘛。幺零二。哦，那逗号刚刚被截断了啊。都要被截断了。加个七。加个七啊，因为它这里面有一个特殊字符啊，一个特殊字符。把长度给它变长一些，它加了那个特殊字符之后，

这个长度就得往前扩一扩啊。二二二六看到没有？二二二六啊，这个白日依山尽逗号，白日依山尽哦，那个逗号还没还没拿到啊，那逗号拿到要要给他发啊，要给他发。逗号拿到要给它发啊，因为我们这个max lens它算的是编码之后的长度啊，要注意一下，它不是编码之前的长度，它是编码之后的长度。编码的时候，其实它的开头和这个屁股后面会加一个东西，

它会有个幺零幺和幺零二啊。你看啊，这是我们刚刚自己数的那个位置，你看大家注意对照看一下这个编码，最后结果是不是跟我们刚刚讲那个结果一样的？所以你们先把第一个重点记住。token字符编码其实逻辑也非常简单，它怎么把一段文本变成一段数字呢？就是去寻找这个vocab字典里面，你每一个字符所对应的这个索引下标。把这个索引下标输出，这就是我们编码之后的一串数字了，明白了吧？那你会发现现在人家这个编码，它会加一些special talkings，

因为我们这里面给它设置为true了。add special talking设置为true了，看到没有？那么这个文本在进行编码的时候，它前面和后面会有两个特殊字符。好，那么这两个特殊字符到底代表什么意思呢？我们可以把编码之后的文本进行解码啊，我们可以把它解码回文本，接下来看一下啊。我们解码这个文本对照去看，它会更加清晰。解码文本数据。好print解码，解码怎么来解码呢？

调用这个talk talking点，它有个方法叫做decode。啊in code是编码，那么decode就是解码，我们解码就一句话，一句话解码哈，拿这个out。out那么out里面我们要解码的，就是人家这个编码之后的结果啊，结果就是input IDS。啊，对这个input IDS。input IDS，因为它input IDS IDS现在是不是有两句话对吧？有两句话我们分别解码哈。

input IDS的第一句话零索引零来进行解码。啊来进行解码，然后。好，接下来是这个talking。这句话第二句话啊，点decode点out。呃，点decode啊，我们这个out。input IDS 1啊，我把这两句话这个分开分开解码好，我们这对照看一下啊。就是你可以把它编码成数字，这个数字同样的也可以转回来，

那么参照的就是这个，我这个表啊，参照的就是这个字典的这个编码表。看到没有参照，就是这个字典编码表，然后大家会发现啊，这个这个地方我们添加这个special talking之后。每一句话它都会有个什么东西，都会有个开头和结尾。啊，看到没有？都有个开头和结尾，这个cls就是代表的是我这个文本的开始。sep的话就代表这个文本结束了，就这句话结束了。

那么，这个东西有什么用呢？为什么我们要加这个special talkings啊？其实这个special talkings不是给我们看的同学们。啊，不是给我们看的。它是给谁看的？它实际上是给模型看的。它是给模型看的，就是说我们是通过这个这两个字符告诉模型，哎，这句话要开始了。然后到这个逗号之后，这句话就结束了，明白了吧？

这相当于说我告诉你，开始和结束告诉模型，开始和结束。不然的话呢？这个这个nlp这个模型在使用的时候就会存在一个问题，它会不停的输出。听清楚啊，它会不停的输出啊，它会不停的输出，你必须要告诉它什么时候结束，什么时候开始，它才知道这个文本的文本的这个起始点啊。所以这两个东西一般是必加的啊，一般是必加的，这个special talking 1般是必加的，

就是这这两个东西没有这两个东西的话，其实我们的文本是不健全的哈，模型没法训练啊，知道就行好吧。好，还有一个细节，注意啊，这个max lens它指的是编码之后的长度，不是编码之前的长度啊。OK，好。好，那么这是第一个参数啊！input IDS其实最核心的就是它。啊，

对于我们来讲，它输出的最最重要的结果就是这个编码之后的结果。input IDS那么接下来我们看一下这个东西，叫talking type IDS。什么叫talking type IDS呢？其实在当前的这个编码里面哈。呃，我们现在看不到这个区别，为什么因为它输错两句话全部都是零啊，注意听它输错两句话全部都是零，因为这个talking type IDS不是给我们现在用的这个方法用的。但是我把含义给你们说一下啊，如果你们以后做其他任务的时候涉及到了，你应该可以反应的过来。就是这个参数的含义是什么？

它是第一个句子和特殊符号的位置是零，第二个句子的位置是一，它有个前提是这个东西只针对于上下文的编码。啊，注意，它只针对于上下文编码啊，上下文编码什么叫上下文编码？比如说早期我们用来做这个问答的这种操作。啊，问答的操作问答就是一个上下文的关系，我的问题是上文答案是下文明白了吧？问题是上文答案是下文那么问题，我们会。按照一种方法进行编码，答案会按照一种方法进行编码啊，

是这么来做的，但是现在这种方式呢，已经基本上已经被。抛弃了哈已经被抛弃了。啊，注意，已经被抛弃了，因为这种东西有个最大的缺陷在哪呢？我们得对每一个这个数据单独做操作。它的通用性不强啊，所以说这个参数我们现在看不到效果哈，因为我们这儿用的这个编码方法不是按照上下文编码的，我们这儿是批量语句编码用的，不是上下文编码啊，这儿看一下这个参数就行了啊。

啊，那么下面这个东西special talking mask。它就比较好理解什么叫special talking mask呢？special talking mask代表就是我们特殊的这个字符所在的位置，看到没有？啊，你比如说我这句话里面编码之后是不是有个开头一个结尾有个特殊字符，看到没有，它用一来代表特殊字符的位置啊，开头和结尾。看到了吗啊，开头和结尾啊好，这是special talking mask，它代表是特殊符号的位置啊。很好理解，

特殊符号位置，那最后一个lens就不用说了。lens指的是什么？lens指的是编码之后的长度。呃lens lens是编码。之后的长度。其实这个编码之后，这个序列长度就跟我们刚刚要设计的这个东西是一样的啊。好，这是我们的这个文本编码啊，文本编码。呃，然后我给大家看一个比较特殊的东西，注意看。我们刚刚这设置了一个厂，

开始截断了是吧？这个最大的长度设置是八。所以你看啊，第一句话长度编码之后刚好是八。啊，所以第一句话是完整的，发现没有第二句话。它到这个这个位置的时候。再加上加上一个开始和结尾，是不是长度已经超出八了？那么超出八的部分就被干什么就被砍掉了？明白了吧？就被截断了。好，那么如果说我现在把这个长度放大一点啊，

比如说我们给它放成这个15。大家再来看一下它的这个编码的一个特点啊。超出超超出超出食物的食物的部分会被截断，但是如果你这个文本不够15长，那怎么办呢？我就用零去补齐它。看到没有？你看现在白日依山尽后面是不是多了一堆的这个pad pad pad？看到没有？啊，并且注意看啊，这个sep是加在补齐的前面的，为什么？因为我们现在已经发现你在补齐之前，这几句话已经是不是已经结束了？

明白了吗？所以补齐是往后补的啊，补齐是往后补的，然后你看这个input IDS后面是不是就是按照零去补齐的？把这个特点也要记住啊，这是补齐的方法啊，补齐的方法好，那么如果说你的长度不长度超出了，那就截断了啊，那就截断了。那就是补齐跟截断啊，注意一下这个特点就行了，好吧好，那么这是talking的这个语句编码啊，语句编码。

好，那么我们这这个talking的核心作用就是什么呢？把文本变成是一串什么数字啊，重点就是这个input IDS，因为有了这个东西之后，我就可以把这串数字。传给模型，并且这串数字是通过这个什么vocab啊vocab。将文本一一对应一起的，所以本质上来讲，本质上来讲。你这句话。啊，我们上面的这句话和上面这句话是完全等价的。能理解吧啊，

是完全等价的，你可以把它想象成是下面是中文，上面是什么？上面是机器的这个语言啊，就是我们这个大模型的这个语言。你们就可以理解到了啊，就是中跟中英文切换是一样的道理，它是完全等价的啊，完全等价的好，那么这个东西给到模型之后呢？其实难的地方不在这，同学们难的地方不在这啊，这个数字给到模型之后，这个数字我打个比方，你比如说。

白日依山尽，这句话它能用这个四六三五三幺八九八九八二二五二二六八零二四直接来定义嘛，那肯定。不行诶，那肯定还不得行。因为这串数字只是我们人为定义的啊。这是我们人为定义的。计算机打死他也不知道这个四六三五是白。能明白我意思吗？他是不知道的，他是不知道的，那么怎么让计算机能够理解到？这句话的含义或者说直白一点，这个东西是词或者语句对吧词，它有它自己的词性。

有性质在里面。怎么让计算机能学到这个东西呢？其实我们没法告诉他啊，注意重点在于我们没法告诉他，他得自己学。他得自己学。明白了吗？它得自己学，它得自己从大量的数据中去学习。这串序列到底代表什么意思？那么，这个学习之后就是学成以后啊，它会输出一个高维的特征，这个高维特征才是词向量。啊，

这个高维特征才是词向量哈，这简单听一下就行了，什么是词向量就是我们上节课给大家打过那个模型，模型里面有个embedding，那个embedding就是输入这串数字。经过大量的文本学习之后呢，他就能够理解这串数字代表白日依山尽这个含义了啊，所以说这个中间会有一个很复杂的操作。啊，你们得得明白啊，他不是说说是哦，老师给我们讲了，今天晚上艾瑞老师给我们讲了对吧？哎，我们的那个文本转词很简单，

就是找个文本，找个字典，然后。给它做一个索引就可以了，那不是的，那只是个开始啊，真正复杂的东西在它里面啊。好，但是对于我们应用来讲，这个是核心，听清楚哈，从开发的，从人家那个做算法开发的角度来讲的话呢，它那个词向量的那个模型是核心。但是对于我们做应用开发来讲的话呢，

这个地方是核心，因为我们重点是要搞明白。怎么把文本转成模型，可输入的数据？听清楚啊，怎么把文本转成模型，可输入的数据，这个玩意是我们做应用开发的核心好。OK，这是我们的第一个重点啊，就是这个。token啊，token编码啊，token编码OK，那有了这个我现在知道把文本转成模型可塑的数字之后，

其实就已经够了，接下来我们就得。处理我们的这个什么小项目啊，我们这个小项目，我们第一个这个小项目就会给大家开始介绍AI项目开发流程。AI项目开发。流程这个东西得记住啊，以后所有的项目都是按照这个流程来开发的AI项目开发流程，它是有一个模板可套用的。我们所有的AI项目都分为这么几个点，第一个点就是需求。需求或者我们把它称之为是数据啊。需求的目的其实就是为了搞明白用户想要干一件什么事啊，那么我们跟用户沟通的时候的话呢，我们就得拿到他的这个什么数据。

拿到它这个数据好，那么这个数据拿到手之后的话呢，你得做很多事啊，我们待会给你们说这个事情包含哪些东西，有了数据之后，接下来下一步我们就是这个什么模型？模型啊，模型因为模型一般我们现在做的事是选型。啊模型我们一般是选型或者简单来说一下，就是简单的这个什么设计选型或者设计啊，但是设计其实我们做应用开发，现在设计不到。只是我们前面两个项目里面会涉及到一些简单的一个设计操作，后面的这个大模型的训练过程中，其实都不需要我们自己来设计了啊。

好，那么第三个阶段就是模型的训练模型训练。啊，模型训练那么最后就是效果评估。效果评估啊，那么再往后面走，还有一个部署啊。还有个步数，那记住啊，所有的AI项目流程都是按照这个五个阶段来进行。推进的任何AI项目都是这样的，那任何AI项目都是这样的，好了，我们先来看一下，我们今晚用的这个数据啊。

数据怎么来获取呢？我们说一下这个数据，数据的问题啊，说一下这个数据问题，我们现在用的数据的话呢，就用的用人家这个大模型平台上面给我们提供提供好的啊，提供好的。好data。test啊，data ta test我们这啊，这个地方我得开一下那个啥。因为我们现在haagen face啊，haagen face得开一下，不开的话呢，上不去。

咳咳。往上啊，往这看一下这个。开了啊。啊，我说过啊，这我们前面这几节课其实是最复杂的。比你们后面学的所有东西都要复杂啊，所以说一定要下去，一定要认真的去操作啊。数据集的话呢，我们可以通过现在大幕型平台就可以去来获取，你看haag face里面它有dataset看到没有？啊，有个dataset，

那么这个dataset里面就可以找到我们想要的这个数据啊，我们想要的数据，你比如说我们找这个数据。我们找一下这个数据。就这个数据，看到没有？是它的啊呃，就是它的。哎，这个没卡片啊，我们换一个人。换个有卡片的诶，这个就有卡片看到没有啊？这个就有卡片，这是个什么数据呢？

这个中文的。情绪评价的数据。明白了吧，中文的情绪评价的数据，这个数据集就分成这个数据结构非常简单，它分为两个部分，它这个分类数据首先注意它这个文本分类数据。它的数据集分成两个部分，一个是文本。另外一个是标签。标签有两个类别，零和一。看到没有啊，标签有两个类别，零和一。

零代表的是负向的情绪negative，一代表的是正向的情绪positive。好，那我们来读一句话试一下啊，你看书看到最后几页居然是双眼皮，所有的字都印成了两。两遍头根本就不能看输的质量实在太差，这很明显是个差评嘛，看吧，有些东西不赞同事后的捷径不过如此，年轻人该经历的还是要去体验。否则拥有后还会抛弃，因为那不是他自己选择过的，所以此书可。是说有误导成分，

那个很明显也是差评，对吧？那么看一个正向的评价啊，是一个朋友送给我的，如今我又把它推荐给其他朋友。是他为我打开了一扇门，通向什么什么的门啊吧啦吧啦一大堆感谢的作者怎么怎么样那那那这个情绪就是这个评价就是个什么正向的嘛，看到没有啊，就这个意思。啊，我们这儿就直接借助于拿这个开源的这个数据来用就行了哈，那么如果作为一个这个真实的项目来讲的话呢，我们到后面会给大家带着大家来来去，就是后面后面我们做这个规模稍微大一点的项目呢。啊，

我们会教大家就是这个数据，自己如果收集到原始数据之后怎么做这个处理啊？怎么做处理？因为你最终要把数据变成这个样子。这个数据变成这个样子，它得经过前面几个阶段，第一个阶段叫做清洗。什么叫清洗呢？就是我们要把一些错误的，有缺失的，或者说跟这个主题无关的数据给筛掉。啊清洗完了之后，第二件事就是标注。啊标注你，比如说人家这里面标注，

你看这个就是他打的标签嘛，对吧？我最简单的标注方法就是我做一张表格啊，你把每句话放在里面，然后你后面给我填零和一就行了。负向的填零，正向的填一，就这个意思啊，这是人工标注，那么这儿有一点我要提前说一下数据的部分。就是人工去做的。它是逃不开的啊，所以说AI它有一大部分的成本是花在数据上面啊，另外一大部分的成本就是花花在这个设备上面了。啊，

就是我们训练模型得有设备啊，训练大，尤其是训练大模型得有设备啊，成本核心成本就在这两坨哈，人工啊和这个设备。好，那我们现在就先先来搞我们的这个案例啊，我们就用这个数据集啊，就用这个数据，那这个数据集我怎么来给它获取呢？很简单啊，我们可以直接通过这个嗯hagen face里面，它所提供的一个类，这个类叫做dataset。from dataset.

import.啊，它有个方法叫做lo add。啊，一个load data set。好，我们在这可以在线。加载数据。啊，在线加载数据。dataset就等于我们的调用这个load data set，然后里面有个path，这个path传值传哪个值呢？就传我们刚刚这儿看到人家这个数据的名称，把它复制一下啊，

把它拷贝复制一下。丢进来，丢进来。好，然后这个玩意儿呃，其实我跟你们说一下吧，然后这个上面应该是有获取方法的，我觉得啊诶，他居然没写吗？data studio我看data studio里面。啊hang face里面现在还没写，没写这个。呃，它没写这个获取方法啊，那个魔塔社区里面有啊，

哈登fish里面这个卡片里面还没写啊，没写呢，我觉得你就用我们的代码就行了哈。它这里面是这样的，就是我们可以可以通过这个path存储path去寻找我们要找的这个数据集。明白了吗？然后这个split是什么意思呢？其实我们需要对这个数据做分割啊，但这个split你可以不写哈，可以不写。嗯，不分割也行啊，只是说它如果需要分割，我们再分割就这一就这一句话就可以了，明白了吗？

就这一句话就可以了啊，它就可以把我们的数据集给我们加载进来。呃，就可以把我们这个数据集给我们加载进来，好，我们这儿加载看一下啊。print一下这个dataset。等它加载完之后，我们看一下啊，我们右键run。这就是在线的去获取这个数据，那么当然了，证明有个前提条件是什么呢？你的网络得能够访问到谷歌云盘，因为。

给hard face上面所有的数据集基本都是放在谷歌云盘上面的。你的网络得能够访问谷歌硬盘啊，不然的话下面会报一个错误，叫做http connection error啊，就是网络连接异常。像这种警告不用去管哈AI，上面我们遇到这个警告是很正常的，这是家常便饭啊。如果你觉得看不惯他的话呢，你可以搜一下怎么去屏蔽警告的，但是我觉得没必要啊。嗯，怎么异常退出了？我终于下去了。这个网络不通吗？

换一个代理试一下。换一个代理试一下，我看我挂的这边是美国的呀。数学有错吗？等一下，我换一个代理试一下啊，换一个换一个换一个。582延迟这个。呃hag face平台最大的一个问题就是这个玩意儿对我们国内不是很友好啊，我们从它上面去搞点数据。很难搞模型啊，搞数据很麻烦哈。啊，经常下不下来？但也有可能是它这个资源的问题。

环境应该是对的啊，等一下我试一下，我换个手机试一下啊。换个数据试一下啊。嗯，排查一下，这个应该应该不是我这边代码的问题，应该是这个它那个资源的问题，有时候是网络问题，有时候是。那个资源的问题，它有些数据集在我们这边就是下不下来。你们下载好了啊。那下载好了就行。但是你你你看没有我这边注意啊，

我给你们说一下。你看我，我去下这个数据就可以下下来，看到没有，然后这已经下下来了。看到了吗？这已经下下来了。啊，它这已经下下来了，但是我我下刚刚的数据就不行啊，我下刚刚的数据就不行就不行，这个跟每个人的网络是有关的。就是哪怕你有时候挂了这个VPN代理都不行啊，都不可以的好啊，这个注意一下就行了，

那么这个数据经会下在哪儿呢？我说一下。你们得知道它的位置啊，就是我们没写缓存文件的时候，它默认在这儿。在你的这个C盘。C盘用户。你当前登录的账户里面有个点catch。然后下面有一个点哈根，有个哈根face的文件夹，看到没有？哈根face里面有个，如果是模型，你没给那个缓冲路径，那它就在models里面。

数据集就在dataset里面。啊里面你看这是我刚刚下的这个数据啊，我刚刚下的这个数据，我把它删了吧，这个应该之前有下过啊，之前有下过，我再试一下。我重新重新试一下啊，它会把那个缓冲的路径给你打印出来啊。呃，我看有同学问了一个很有趣的问题哈，就是说。老师，这个代码很短，怎么知道是从哈根菲斯里面下的？

因为这个包就是哈根费斯的包。嗨，就是我们上节课装的那个包啊，这个包就是hank face里面的data sit啊，这个包是哪个包？这个包是这个包？是这儿，这里面有个docs。这个docs里面有一个dataset。看到没有，因为你用的就是他的东西啊，你用的人家的框架，你下数据，你不从他那下，你从那下的。

对不对啊？你看这个，我这个资源我就可以下下来。嗯，所以到时候看你们个人的情况吧哈，如果数据下不下来呃，那咋办呢？你就你就用我们在线传的啊。用我们在线传的啊，在线传的好，我就把这个数把这个代码就放在这好吧，这个代码就放在这吧。然后刚刚下下来一个数据集，我把那个下下来数据给你们看一下。这个数据集啊hang f上面这个数据集格式是它自己专有的。

呃，看到没有格式，它自己专有的，它下来数据集是一个点arrow的一个格式。呃，这注意一下啊，它下下来这个缓冲数据的格式是一个点arrow的一个格式啊，是这样的。这是它自己专有的一个数据啊，专有一个数据嗯。嗯，我这边这个网络云这个数据在这下不下来啊，那我就把之前下好的数据我就拿过来啊。我拷贝一份，这我们新建一个，

这个叫做data。那我之前缓存的应该是另一个数据集。呃，它两个应该没没有多大的区别，我之前我这说一下啊，我刚给你们看的是这个，我之前缓存的是。是哪个？就这里面的一个，我看一下那个名字叫叫叫啥来着？呃cns group chin呃，就这个啊。啊，我之前应该下的是这个啊，我我刚刚缓存的是这个啊，

我之前缓存的是这个，就是我本地以前缓存过一份啊。因为我这边网络问题，我就不缓存了啊，我就直接用那个什么我之前缓存好的数据啊，我之前缓存好的数据，你们也可以直接用我这个数据集。OK，那么缓冲好这个数据拿下来就是这个样子，看到没有？它是点arrow格式啊，点arrow格式。这个缓存我说一下啊，你们可以给它指定一个缓存的路径，明白没有？

你比如说我这边又给它放到这个data下面去啊data下面去。好，那么这个数据集就会默认缓存到缓存到我们这个data目录上面哈。OK，那么这个缓冲数据我现在有了之后。这个缓冲数据我有了之后的话呢嗯，我们怎么来进行加载啊？怎么来进行加载来说一下？啊，加载缓存数据得用到另外一个包，叫做load。load from disk.啊load from disk。看一下啊，加载。

缓存数据。加载缓存数据。好dataset。dataset就等于我们的load from disk。呃，注意啊，我这说一句。虽然说这个load data set你跑完一次之后啊，你跑完一次之后，它会把数据集缓存到这个位置。但是第二次跑的时候，如果你还是用这个方法，它一定是先要干什么联网去获取一下，如果你的网络是中断的，它还是会报错的，

明白了吧？但是你说我本地已经有了这个缓存数据了，我不希望让它再进行联网检索了，那你就用load from disk去进行加载啊load from disk去进行加载。OK啊，我们就来教大家这个数据集啊，数据集这个东西的话呢，我们一样的你你给到哪个路径啊？注意听。数据集给到哪个路径呢？任何一个数据下来之后，它都会包含一个这个json文件啊，json文件我们给到这个json文件所存放的文件夹就可以了。啊，拷贝一下这个路径，

这个路径一样的也要给绝对路径啊，也要给绝对路径啊，不要给相对路径，因为因为一给相对路径的话呢，在这个哈根菲斯的这个平台上面它。默认会去寻找它的那个云盘所存储的一个位置啊。好，我们来print看一下这个dataset。啊perl，看一下这个dataset。你print的时候，它其实输出的是什么东西？输入是这个dataset的一个属性，目前这个数据分为三个部分。啊，

分为三个部分，三个部分，分别是training validation和test，就是它把这个数据分成了三份。明白了吧啊，分成了三份啊，分成三份，这三份分别是用来做训练的，验证的和什么测试的？啊，那么训练里面我们可以看到它的数据的特征，构成分成两个部分，有文本和标签，一共有9600条数据。验证和测试里面都是1200条数据啊，

1200条数据，这是切分之后的数据集啊，切分之后数据好，那么我们现在想看一下这个数据的内容怎么来看呢？我们可以遍历这个数据集来查看啊，比如说我们这儿看一下这个training data。training data我看下这个训练集对吧？就等于我们的data data set啊dataset好，你通过这个字典的这个键来进行索引。确定，然后。我们去遍历一下这个什么呢？嗯，数据集啊for data。in training data.

然后把每一条数据取出来看一下啊，print一下这个data。print一下这个data啊。我们接下来遍历每条数据来看一下。哎，这就把它的这个什么呢？这里面的数据集，我们就给它打印出来了，看到没有？跟人家官网上面展示的是不是一样的？它每一条数据就是一个字典啊，就是一个字典，那么这个字典里面包含了两个间隔段，一个是文本，一个是label啊，

一个是文本，一个是label。好，这是这个数据集啊，这个数据集我们就拿到了，转换过来了，好，那么题外话啊，多说一句。这种数据集啊，这种数据集它是hagen face专有的，我们一般情况下不会把数据集做成这种加密的格式，我们一般做成什么格式呢？一般是csv格式的。啊，

我给各位说一下啊，我们一般的数据是csv格式的。啊csv格式的呃csv格式的，我找一个csv格式的数据给你们演示一下子这个吧。啊，找个这个数据啊，我前面有一个这个数据看一下，这就是一个csv格式的数据，当然它不是个中文的数据啊，这是个英文的数据啊，英文的数据。我们一般的数据可能是这种的啊，这种的那么如果是csv格式数据，我能不能借助于的这个平台来进行加载呢？也可以也可以啊，

注意也可以，并且它这个数据集是可以转转换成csv格式数据的，但是它转不回去。好，我说一下，如果是csv格式数据，怎么来加载啊？这是一个扩展啊扩展。加载哎，加载。csv格式。数据加载csv格式数据用的还是load data set。它就又特殊了啊，又又又转回去了啊data sit。dataset就等于我们的load data set。

这个方法要记住啊，它的path现在给的就是csv。pass给成csv，然后有一个data files的一个参数，这个data files参数现在就给你的这个csv的绝对路径。呃CS这个data file里面里面就给csv的这个绝对路径啊，给这个绝对路径，它就可以来加载这个csv数据了。呃，等一下把这个先停掉啊，把前面的先屏蔽掉。免得打印的东西太多啊。所以它这个load dat AC的体系其实功能很强大啊，它可以直接处理我们的这个csv格式的数据啊。看到没有，

这是不是也也也把这个数据集加载出来了？看到没有啊，也压榨出来了啊。好，那么拿到之后，你如果想要去便利它的话，就跟我们刚刚方法一样啊，我们这儿就不便利了，然后这个特征它就分的比较多啊。看到没有它，它是个多分类的，这个一个数据啊，这个数据就跟我们做个演示啊，那么一样的，就是说我们如果说啊，

注意听我前面因为加载这个数据的时候它。我这边的网络原因呢，加不下来啊，你这个数据集加完之后，你还可以转成csv格式啊。呃，我这再多加一句代码，就是这个数据集转为转为csv，它也可以转成csv数据啊。转为csv格式怎么来转呢？就是dataset。就这个东西啊，但是它必须得借助于node data set进行加载啊，必须得用这个方法加载这个from disc就不行啊dataset点有个方法叫做to csv。就这行代码，

把它放开啊。注意看啊，这个dataset点儿有个方法叫做to csv，看到没有？它可以把就是hand face里面加完这个数据之后可以转成以下这些格式。有CS VC这这这个字典呃，还有json list pandas，基本上你们常用的这个格式都有。能理解吧啊，可以转哈，可以转那它那个加密数据是可以转的，那么我们这就以csv为例啊，你比如说csv我转的话怎么来转呢？这里面给个参数，叫做pass or buff。

里面你直接给你要保存的这个csv的，这个路径就行了，理解没有啊，给你的这个csv路径就行了，比如说我们拷贝一下这个路径。拷贝一下它这个绝对路径。假如说我就存到这个里面，对吧？叫做这个点。tsc啊点csv啊。哦，这不这这说错了，不是点csv你就给这么一个路径。理解没有你就给这么一个路径，这儿不给不不不用给这个点儿csv啊，

你给一个路径，它默认就会保存到这个上面转成一个csv格式一个文件。呃，但是这个地方我们是这样的，如果是WINDOWS下面的话呢，听清楚，如果是WINDOWS下面的话呢，你要加一个点csv。你要加个点csv，你如果不加的去，你如果在Linux系统下面就可以不加啊，那这个跟系统有关系啊，因为WINDOWS打开文件得看你的这个什么呢？得看你的这个后缀美啊啊，就这个意思啊，

这是可以转可以转的啊，可以转的嗯，这个你们到时候自己操作一下啊，自己试一下。哎，这我就给大家。关掉了啊，我们就不用它了。好，那么我们接下来就用这个方法，因为我这边是缓存好的数据啊，我就用缓存好的，这个这个方法来。使用啊，来加载我们这个数据啊，

加载我们这个数据，我们打开这个，我们看一下。我们分别看一下这几个数据集啊，看一下这个训练部分测试部分，还有就是什么呢？这个验证部分啊。看，这是我们的训练集啊，训练集我们到时候就拿这个训练数据，就拿这些数据来训练我们的这个模型啊，来训练我们的模型，然后如果你要拿测试集的话呢，这里面一改成test。呃，

改成test，它就是测试数据。嗯，它就是测试数据。啊，这是加载图的，就是测试数据啊，就是测试数据好，如果你要做验证的话，就加载那个验证的部分啊，加载那个验证部分好，那么这个数据我们现在就有就有了，那么有了数据之后，接下来这个数据你还得做处理。啊，

我们往下走啊，有了这个数据之后，第一获取数据，第一获取数据，第一步我们是获取数据，对吧？这个数据我们已经获取到了，接下来第二步我们得呃转换。数据格式。啊，其实就是制作数据集啊。啊，制作数据那么这个数据格式其实人家这里面做的已经比较标准了啊，做的已经比较标准了，我们接下来得把它转成我们的这个模型，

可输入的这个dataset。好了，我们来看一下怎么来转啊？怎么来转我们这新建一个类呃，新建一个py文件啊。排程发样，这里面我们取名字就叫my data。啊，my data啊OK，我们一起来看一下这个数据的制作部分。嗯，好。如果是我们自己写代码来做数据集的话呢，我们这得调用那个paddle的那个框架。啊，

我们导入一下，导入一下这个torch。点儿有个utl s，下面有一个data，那data import它这里面有一个data data set。啊，一个dataset的类啊，一个dataset类，然后我们得加载我们缓存的那个数据啊，我这边因为网络原因，我就加载那个缓缓存的数据。缓存数据的话，我们用load from disk。load from disk啊，加载滑动数据好，

接下来我说一下制，我们得制作我们自己的这个数据集，那么怎么来制作数据集呢？这个模板是其实是固定的啊，模板是固定的，就是我们新建一个类叫做my datas it。然后让它继承这个paddle ch的dataset。啊，继承这个paddle这个data set paddle这个dataset里面有三个类，我们呃三个方法，我们进行重载，第一个initial。先把框架打起啊，第二个叫做。认识啊，

认识。好，第三个。叫做get item。呃，给它一层啊OK好，先给它pass掉呃，第一个方法用来干什么的呢？这叫做初始化数据集。所有的数据集都是只有三个方法就够了啊。啊，初始化数据集其实就是把数据读进来，从磁盘读到你的内存，第二个方法就是返回。数据及长度。

就是你这个数据集里面一共有多少条数据？最后一个方法是最关键的就是对。对每条数据单独做处理。啊，单独做处理好，那么我们第一步先来初始化啊，先来初始化初始化其实就是把磁盘。把数据从磁盘加到内存啊，从我们这里面就是从磁盘加的数据啊。从磁盘。加载数据。啊，加载数据好，我们定义一个变量来保存我们的数据self点data。点这家sit吧啊。

调用这个load from disk load from disk。把我们的那个呃数据集我缓存下来，那个数据集我们拿过来。就这个对吧？拷贝一下它的这个绝对路径。好丢进来。第一步就可以了，这个数据就加载进来了，对吧？但是这个数据加载进来有问题啊，我们在用这个数据的时候，因为这个数据人家做好划分了。它分为测试，训练和验证对不对？那么我们这就得实现一个功能。

人家要哪个，我们给哪个嘛啊？要哪个我们就给哪个嘛，所以说我们这传一个参数叫split。叫it啊。传一个参数分割的啊，分割参数好，这样写个判断就行了啊if split。呃，if split等于等于我们的这个training。啊，我们通过这个关键字啊来进行判断，如果你传的是training的话呢，我们就给你返回什么呢？这个self点dataset。

就等于什么呢？self点data sets的training的部分。是吧，很简单的逻辑啊，很简单的逻辑好，然后lif lif这个split等于等于我们的这个test。好，那么我们就给这个数据集，让它返回啊，就返回对它的这个测试集的部分。啊，不要用else啊pation里面不要用它啊，我们还是lif啊lif split。split啊，等于等于这个。

呃，验证剂那个验证剂叫？radiation啊，我复制一下。OK，self点data就等于我们self点data就取这个什么呢？验证机的部分就可以了。啊，那么最后。就给它print一个这个。数据名错误。啊，就这样去写啊。好，这就写完了啊，

所以说这个initial里面很简单，把数据加载进来就行了，它只做这一件事啊。只做这一件事啊，好，然后第二第二步啊，第二步返回数据的长度，那也它也很简单，那它也很简单，直接return。直接return你当前的这个数据，要要认识。啊，我们self点data shift的长度就完事了啊，如果要说嗯，

这个代码里面我们一般做数据集比较复杂的地方就在于这个单独做处理。啊，单独做处理，那么我们在这要把这个数据处理成什么样子呢？我们刚给大家讲一个方法，叫做talking。编码啊，叫做talking编码。就是我们最终输入给模型的东西啊，它应该是编码之后的，就前面给你们看过的这种东西，对吧？好，那么现在就涉及到一个问题。我是在这个get item里面去编码的，

还是说在外面去编码的，对吧？其实两种方法都行。啊，你在get item里面也可以做编码，但是在get item编码有个问题，在什么地方呢？注意。我们编码得依靠这个东西，得依靠这个talking。那么talking的话呢，你得去加载这个大模型啊，得去加载这个大模型呃，我们AI开发里面哈，一般情况下不会在这个数据集里面去。

加大模型，因为你加大模型，在这感觉有点奇怪，明白了吗？感觉有点奇怪，所以说做自然语义的这个数据集，如果说我们自己写代码去操作啊。一般的做法是我们在这直接返回文本，听清楚我们在这直接返回文本，然后让模型训练的时候再去做编码，因为模型训练的时候它就会你模型训练的时候你是不是得加载这个？大模型嘛，对吧？你就有了这个talking了，我们那个时候再给你做这个编码处理啊，

这就不做编码处理了啊，一般习惯上来讲这就不做编码处理了，我们这直接去返回文本和标签就行了。所以这个单独处理，我们就做一般情况下，我们这只返回对于自然语言来讲，我们只是返回所需要用到的这个数据啊，好，那么在这里我们用到的数据就是text的内容和这个label的内容，那么分别把它取出来就行了。text text啊，ext。就等于我们的self点dataset好，人家当前这条数据item啊，这条数据的什么呢？

text呃text这个text取出来。好label。就等于我们self点dataset。当前，这一条数据的label。啊，这个字儿不能取错啊，取错的话就拿不出来了。然后我们来返回这个文本，直接返回text。label啊，就可以了，到时候拿到这个text给它单独做处理哈。OK，那么这个数据集我们就写完了，

写完之后一定要去做一个测试啊。啊，写个慢函数，做一个测试。da data set就等于我们的my dataset，我们实例化一个。啊，这儿比如说我们先取下这个测试集啊，取下这个测试集来遍历看一下啊data in这个dataset。print一下这个data，看一下这个data取出来是不是我们刚刚想要的这个文本夹这个标签的形式？咳咳。诶，看到没有，这是我们想。

要的那个效果啊，传test那么它就是test，你如果传的是training它取的就是training啊。啊，写完这个东西一定要验证一下啊，就是说我们得返回我们所需要用到的这个样本，当然了，这个东西在现在的这个大模型微调框架上面是不需要你自己写代码的。因为框架帮你写好了啊，我们去做一个配置文件就可以了啊，这个我们到后面也会用到啊，所以说有些同学可能不是做开发的，当然你们做不做开发？不重要啊，以前做不做开发不重要啊，

因为到后面这些代码都不需要你们去写，你们到后面不需要写代码，我们要的是调框架啊，调框架我们前面的话讲点原理。所以带你们简单写写这个代码哈，去体会一下它的一个完整的处理过程OK好，那么这个数据集就做好了啊。顺序做好之后的话呢，我们下一步就该干嘛了呢？数据一有了，下一步是不是就该设计模型了是吧啊？设计模型好，我说下这个模型的模型的设计哈，模型设计是这样的。我们目前这个项目用的微调，

用的是增量微调。啊，增量微调什么叫增量微调呢？简单讲一下。就是原来的模型是这么大。我在这个原来模型的屁股后面加一部分。理解没有啊，加一部分加到一部分叫增量。呃，加的这一部分就是增加的这一部分叫增量，增加了一部分，我说了前面的这些项目里面它不需要用到gpu CPU，也可以训练，为什么因为我们训练只训练加的这一部分？模型很小，

只要是个电脑都可以跑，明白了吧？只要是个电脑都可以跑啊。OK啊，我们接下来就来设计一下模型，单独写一个，这儿单独构建一个类吧啊。呃，一个p文件，我们起个名字叫做net。啊，就叫net。OK.好来看一下这个模型部分啊。import模型的构建，

现在的模型都是基于paddle ch的啊，包括这个hang face也是基于paddle ch的。好，我们第一件事先来定义设备信息。啊，定义设备信息device。其实设备就两个，要么是gpu，要么是CPU啊torch点device。好，这个代码我们这样来写，让这段代码自动的去判断啊，自动的去判断，判断逻辑很简单，如果当前设备支持库达，

就用库达不支持，库达就用CPU啊。啊cud a if touch点cud a。点is available就是如果当前的环境里面支持库达，那么我们就调用库达，否则就用。CPU啊，否则就是CPU，所以这个代码到时候你们是通用的啊呃，任何同学这个电脑上面只要你有paddle就可以跑。然后我们输出一下这个设备信息啊，输出一下设备信息，接下来我们就得加载英语训练模型。啊，加载预训练模型。

BD啊。就是我们的bert。这得导一下这个模型啊，我们把这个模型导过来，把这个模型加载模型，这个也也导进来啊import。这个from transformers。import我们的这个叫做bert model啊。bert model啊，把我们的bert模型加进来。birth model点from。那奇了怪了，为什么每次到这个里面它就调不到了呢？诶，我得看一下啊。

我们toke na der里面都可以呀。哎，掉不到算了，我这拷一下啊，可能拍摄面有点有点毛病啊，它有时候智能提示就一会儿就没了，当然也有可能是我电脑。开的直播比较开的软件比较多，卡了啊，没关系啊，就我们还是之前那个方法b model点from print，把你的这个模型加进来，模型一定要到一定要到这个config的这个文件夹啊，包含config的这个文件夹里面。而且一定得传绝对路径啊，

好把模型加进来，模型加进来之后，这个屁股后面我们得加个东西。注意看啊，今天这个代码屁股后面得加个东西，这儿有个点儿to括号device。这句话是个什么意思呢？就是我把这个bert模型给它加载到device这个设备上面去。如果你的device是cud a，那就放到cud a，如果是CPU，就放到CPU啊，这个这个后面一定要跟一个这个东西啊，跟了这个东西之后，我们就可以自动的去把模型放到。

这个适用的呃，设备上面去了呃，然后。先加一下吧啊，然后我们把这个模型看一下啊。print这个print trained。先看模型，然后再教各位怎么来做增量的设计啊？好，我现在是哭的啊。当然，我多说一句，我这边教学的这个设备，上面这个卡很一般啊，因为我们到后面用的是服务器。

你看啊，我这边是幺零六零的啊，幺零六零的，我幺零六零上面是可以装扩大十二点四的，我装的就是扩大十二点四啊。所以你们不要去纠结那个什么，你的什么三零六零三零七零不能装新版的酷带都可以装啊，你直接装就行了，这玩意儿没限制啊。只要是4 GB以上的显卡，任何版本的库达都可以装啊，英伟达这一块是没有给你锁死的。好注意看重点。这个bert模型，它分为这么几个部分，

第一个是in bedding，我这再强调一次，因为刚刚给大家不是最开始讲了个talking嘛，是吧？talking我说过了，他做的事儿是把一句话编码成字典中的这个。索引的位置，但是同学们切记这个索引的位置，它只是给embedding模型的一个输入而已。听清楚哈，它只是给embedding模型的一个输入而已，就是我以这种位置的形式告诉这个模型，我输的是这几个字。那么，模型这个模型它能够理解到这句话的含义，

那它得经过训练好我们这简单看一下这个embedding啊，这个embedding里面第一层叫做word embedding word embedding是什么意思？就是把词变成。向量的那个模型。明白了吗？这个embedding就是把词变成向量。变成向量。啊，然后你注意看啊。它输出是768，那意味着什么意思呢？这个隐办理模型的维度是768。维度是768，什么叫维度是768？简单来讲就是一个词，

比如说我们现在词是一个字，对吧？一个词。会被编码成768个数字。模型这个embedding模型是以768个数字来代表一个词的。这768个数字，其实代表的是768维，它相当于说是把我们的这个数据放到了一个768维度的空间中去了。它是个维度的意思啊。那么，为什么要把一个这个呃字符给它放到768维的空间呢？你想我们人才生活在一个三维的空间，为什么要把它放到那么高维空间中中去呢？维度越高，它能够解决的问题就越复杂。

对于特征的这种抽象化，抽象化能力就越强啊。我们这个课我就给你们讲到这儿就适可而止了，再往后讲，你们就听不懂了，你们把这儿能听到这儿大概有个感觉就行了，好吧，就是我们。在词向量编码的过程中呢，会选一个比较高，相对来讲比较高维的空间去表达这个词的词性，因为词性是很复杂的东西。词性是很复杂的东西，词性呢，我给你们说一下这个什么叫词性啊，

就是我们这个词为什么比较难呢？词与词之间，它有相关性，比如近义词，反义词，对吧？包含词等等等，各种各样的关联度。啊，各种各样的关联度，我们要准确的去表达这个词性。换句话来说，就是你得把这个词放在某一个空间中，这个空间中的位置得合理，它不能乱放。

明白了吧，它不能乱放那么模型，怎么知道这个词在？特征维度空间中的位置呢，他得学习，他得训练，那他得学习，他得训练，所以说这个东西是必须得经过训练的，而且得经过大量的文本做训练，他才能够训练的出来。啊，大量的文本做训练才能够训练出来，不然它找不准那个位置啊。啊，

所以说这注意看一下嗯，我们现在为什么看不到单独的词向量模型呢？给各位讲一句，现在的大模型是自带了词词向量模型的。记住这句话啊，这个bird model里面是自带了词向量模型的啊，这是自带了词向量模型，所以说它是把词向量嵌入到自己模型内部去了中去了。然后后面还有几个embedding。还有后面第二个引办点是做位置编码的啊，是做位置编码的，那为什么这个位置是个什么意思呢？也多多讲两句话啊，因为我们今天是晚上，时间有限啊，

而且这个东西跟你们应用其实没多大关系，只是给大家稍微把这个原理给你们稍微探索一点点啊。注意听，我们现在用的这个自然语言模型比较特殊啊，它叫做transformer。transformer transformer模型本质是个什么东西呢？本质是个全连接。加一个注意力机制。加个注意力机制。这个全链接模型有个最大的问题在于，它不具备序列的处理能力，它是无序的。它不具备序列的处理能力，所以说全连接其实是没有办法去处理序列问题的，你比如说文本就是个序列问题。

文本就是个序列问题，全链接是无法处理序列问题的。那么，怎么让这个全连接能够去记录序列信息呢？对位置进行单独的编码。就是通过这个position embedding实现位置编码，就可以把文本的序列记录下来了，所以这个玩意是为了弥补全连接神经网络所。不不存在的那个缺陷的啊。好啊，这是两个，其实如果说是研究这个自然语句里面哈。它最核心的研究问题包括自然语nlp的这个呃算法开发啊，因为大家在这学的是你们学的叫应用开发，不是算法开发哈。

算法开发里面，它最复杂的，最难的部分就是word embedding和position embedding，那因为这个东西啊，我我我这稍微讲一点这个东西，它会涉及到。它会涉及到很复杂的，很复杂的数学。很复杂的数学啊，很复杂的数学原理。然后这个word embedding里面呢，它也会涉及到这个线性代数的高维空间的特征。嗯，我们叫做相似矩阵啊，高维空间的相似矩阵的这个问题啊，

包括特征分解的问题啊，所以这两个东西其实很难啊，很难就是我们应用上面来讲的话呢，大家知道它的作用就行了哈。就是人家这个word embedding做的事呢，是将我们刚刚的那串数字变成了768维的词向量。position embedding呢，是给这768维的词向量编码对应的位置啊，那么这样一来的话呢，就可以解决。全连接神经网络无法嗯处理序列的那个问题啊，以及就是说我们如何用一串数字代表人家这句话的含义的啊，就这个意思。好，当然这个东西知道就行了啊，

我们再往下走啊，再往下走。呃encoder这个部分呢？是干什么事的啊？就是说上面经过envi ding模型得到了词向量encoder部分就是。进行特征提取，就是来学习特征的啊，学习这个词的特性的好，我们现在要操作的是什么东西，注意看啊。这个哦，这个bert模型啊，它的屁股后面跟了一个磁化层。看到没有？跟那个池化层池化层最后一层的输出就是池化层，

这个叫做bert polar啊，池化层最后一层输出768维的特征。啊，它是768维的特征。啊，768位的特征好，我们接下来要做的事是这么一回事。你看注意看啊，我们今晚用到的这个数据。再给大家看一下我这个数据集。我们今晚用到这个数据呢，是个二分类的数据。看到没有，我们今晚用的这个数据是个二分类的数据，就是我们要的结果就有两种，

要么是正向的，要么是逆向的，对吧啊？要么是正向的，要么是逆向的。所以我们要的输出结果就是这样的一个，我们现在要的模型是这样的，就这个模型最终输出的结果为多少，最终输出结果为二。柱子最终输出结果为两个。一个代表零，一个代表一零的话就是负向的，这个评价一的话就是正向的评价，所以接下来我们得设计啊，这个模型因为这个模型默认的输出是768。

看到没有默认的输入是768好，当你默认输入为768的时候，我们怎么办呢？我们怎么把这个768变成二呢啊？我们可以往这个模型的屁股后面增加一层。增加一层，我们把增加的这一层叫做下游任务模型啊，叫做下游任务模型。啊，下游任务模型，这个任务模型就我们自己来设计，追加到这个嗯源于的b模型的后面就可以了。好，那么我们我们来设计一下啊，我们来设计一下，

接下来我们来设计这个。设计这个模型的这个下游任务啊。定义。下游任务。呃定义下游任务，这就是我们今晚所说的这个增量模型啊。增量模型啊，这是我们自己来设计的好，我们自己写模型怎么来写呢？我们就写一个类啊，叫做model。让它去继承这个touch点nn。点models啊，paddles里面它对这个AI模型有一个定义的父类，继承它之后的话呢，

我们重载它的两个方法，一个是initial。这里面是做模型初始化的，因为你这继承了这个。n点model里面方法，所以说我们在进行初始化的时候，先得初始化它的这个什么父类啊super。啊，现在去画它的父类，父类里面的这个in接首。父类里面有一首好。我们在这就来设计这个嗯，增量模型，我们现在要做的事很简单，是个二分类任务，

对吧？我们就设计一层全连接。呃，设计全连接网络。实现二分类。任务啊，实现二分类的任务好self点FC啊，我们这来写一个全链接啊，全链接的话，你调用这个torch下面有一个包叫做nn包，下面有一个。类叫做layer啊，这就是全连接神经网络。就能也使用了啊。那么，

这个全连接使用网络的话呢？这个模型有个输入模型，输入是什么？注意看。刚刚说过了，bert在前面对吧，我们后面给它增加一个部分，所以我的这个模型的输入就是bert的什么输出，它是串在一起的，能理解吧？我的输出是二输入为多少？输入就是人家上一层的输出，它上一层的输出是768，所以这个模型的输入是768，输出是二啊，

输出是二。啊，这就值来写768。到二就可以了。就完了啊，就完了啊，我们这就设计一层就够了啊，设计一层就够了，因为本身这个任务不复杂啊，好，接下来我们再写模型的前向过程。in击手里面只是来设计模型，那么这个forward呢？是干什么呢？是使用模型。

啊，处理数据。我们把这个过程也叫做执行倾向计算啊，执行。前项计算啊，执行前项计算。好，那么这个前项计算是怎么来做的？注意听啊，你的数据先给谁把这个思维搞明白？我们这个模型现在是这样的，它包含两个部分，第一个部分是它原有的这个bert。第二步是我们刚刚自己设计的，对吧？

那么数据是不是先给到bort模型？bort模型处理完了，结果是不是再给到我们自己？自己的模型对不对啊？所以说这个数据。这个数据传进来之后，是先给到bert的，那么如果说你现在要给这个bert模型给到数据数的话呢？你得知道它的输入为多少啊？那么这个输入就是我们前面所讲的talking types里面。编码之后的结果啊，编码之后的结果，人家这要的输要的输入是这几个输入，第一个是input IDS。啊，

它的第一个输入是input IDS。第二个输入呢，人家叫做这个attention mask。attention mask，这是bert模型的所需要的输入。attention mask第三个输入呢，就是一个talking type IDS。啊，talking type IDS。呃，talking type IDS好，OK，就这三个就够了啊。好，但是注意注意啊。

什么叫增量微调？什么叫微调啊？什么叫微调？我先说个微调微就是微小的意思。微小的意思就是我只调很小的一部分，我只训练很小的一部分，那么言外之意是。增就是我们今晚的一个增量微调，就是我们不是训练这个bert，这个bert不参与训练，听清楚bert不参与训练，只训练谁呢？只训练我增加的这一部分模型。这就叫增量微调，就是训练的是增加的这一部分啊，

训练是增加这一部分，所以我们在这把。这个bert给冻结了，让它不参与训练哈，冻结bert。冻结bert模型。的参数。让其不参与训练。啊，让其不参与训练好怎么来冻结呢？我们来定到一个代码块里面哈with touch。点no grad。就是让它不参与这个反向传播算法啊。with touch点no grad。把这个代码块放到这个，

把我们需要冻结的参数放这个代码块里面，它就不会参与到训练啊，不会参与到训练。好调用我们的这个预训练模型print对吧？这个print里面将来我们添入。呃，填入它的参数，这要通过专有参数的方式来传参啊！input IDS=input IDS。然后是attention mask，就等于attention mask。完了之后就是呃talking type IDS就等于我们的talking type IDS。啊，这是它这个模型的输入，给到这三个值就行了啊，

给到这三个值就行了，那么得到的就是一个什么output？啊，得到一个output好这一块不参与训练，那么我们参与训练的就放到外面去啊。好，接下来是增量模型。参与训练啊，增量模型参与训练。啊，就把它放到代码外面外面去啊out就等于我们的增量模型是我们刚刚设计的这个FC self点FC，就等于我们的这个。啊，不是等于啊，给它传进去传谁呢？

是不是传这个out对吧啊？传这个out，但是我这说一下啊。这说一下，因为神经网络算法里面的话呢，这个东西它输出的结果是。ns v的。它输出的序列特征，它输出这个序列特征，我们要的只是它最后的这个序列特征，所以说这里面其实我们传的不是out，传的是out点。叫做last。呃，last啊。

这没提示啊，黑灯。呃，state。last hidden state里面的最后的一个序列。啊，你们记住这段代码啊，这段代码的意思就是我刚刚所说的嗯，因为目前为止我们的transform模型，它还是沿用了以前rn的那套。数据的模式，它对数据是ns v的一个要求。n是批次。s是你数据的这个序列长度。v的话就是你的这个数据特征。

啊v的话就是你数据特征嗯，这听一下就行了好吧，这听一下就行了，就是这段代码意思就是我们取的是最后的一段序列长度的特征。为什么举最后一段呢？因为序列信息很特殊，最后一段的序列是包含所有完整信息的，你知道这就行了，好吧啊，你知道这个东西就行了哈。因为我们后面的话，其实不会涉及到编码的问题，这个编码写在这的话，我们简单做个解释啊，它取的就是最后一段序列的特征，

因为最后一段序列特征是完整的信息啊，完整的信息。好，我们这就拿的是完整特征，好，最后返回这个out就可以了。return这个out就可以了。好，那么这个模型我们这就设计完了啊，设计完了。我们今天这个场景就会用到呀，呵呵，我看有同学问说老师为什么设计成增量的什么场景会用到今天这个场景不就用到了吗？对吧？当然，

我知道你的意思啊，我们先把它跑完啊，跑完给你们解释啊。微调就是微微调整一下，什么叫微调就我有少量的数据啊，问了我这说吧，不拖啊。为什么要用增量微调？其实，增量微调是微调中的一种，你记住哈，是微调中的一种，那为什么要用微调呢？就是当我的数据比较少。但是我希望拿这个少量的数据得到我的一个应用的时候。

因为你的数据量很少，你不可能做这个模型的，这个完整的训练啊，不可能做模型完整训练，数据太少了。所以说你训练一个完整的模型，它是无效的，怎么让它有效呢？我只训练这个模型的一部分，这就叫微调。啊，这叫微调，那么训练这个模型呢？一部分有什么好处呢？第一。

快，很快就会有效果，我们待会实验上面，你们就可以看出来哈，第二第二。它对设备的要求低。就我说过了，你不用什么服务器，你一个CPU都可以跑啊，你一个CPU都可以跑，就是这种场景下面我们就会上这种什么增量性的微调啊。OK，模型就搞定了啊，模型我们这的模型很简单，基于人家的这个什么呢bert的预训练模型，

我们后面增加一个。部分这样一个全链接，其实这个全链接在这做的核心任务是啊。我这给你们把这个重点说清楚。你们还记不记得上节课？我们看过这个布尔图的分类，布尔图的分类，它那个分类其实没法处理，我们自己的数据对吧？但是我现在呢？要的业务场景是什么？我做的是不是一个啊？这个上面我没写啊，我做的是一个这个评价二分类的一个任务嘛，对吧？

所以说任务就得我自己来定。明白了吧啊，任务就得我自己来定，而且这为什么是二呢？因为我们做的是二分类，这就是二，如果你做的是十分类的话，这就是十啊。明白了吧啊，就这个意思OK好有了模型了，下一步我们就开始训练啊，开始训练训练没什么特殊的。训练没什么特殊的，就是加载模型开始训练啊，加加载模型和数据。

呃，加载模型和数据。开始训练。其实，训练过程中得观察它的这个状态啊。啊，但这个观察状态，你们先听一下啊，我们一天讲不了那么多，后面慢慢你们讲这个东西啊，其实训练过程中最核心的是观察状态啊。好，那先不着急，我们先训练它，训练起来之后再给你们说怎么去观察它这个训练的状态啊？

好，新建一个这个patio n file叫做training。呃，叫做training。啊，叫它训练模型。这是我们的模型。训练好模型训练，里面需要导入一些包哈，我把这个包就直接这拷贝过来给你们说一下。拍照时肯定要要的，对吧啊？因为我们要导模型嘛，然后这是我们刚刚的数据啊，我们刚刚做的这个数据集。

这个东西是用来干什么的？是用来加载我们的数据集的啊，用来加载我们的数据集的呃，然后这是我们刚刚设计的模型。啊，刚设计的模型，这个是我们的分层工具，这个是训练模型的优化器啊，训练模型的优化器。好呃，接下来跟刚刚一样，第一件事啊，你训练的时候你得定一个设备。啊，我们把设备信息定义过来。

定义设备信息，然后呢，模块性的训练，它跟你们以前做编程开发不太一样，它不像你把代码写完了，这个东西就搞完了，不是的。代码写完了，对于AI开发只是开始啊，它真正开始其实是从开始训练开始的，训练不是一次训练好的，所以我们一般需要定义。训练的轮次。定义训练的轮次轮次啊，轮次epoch。

呃，这个轮次的话呢，给个大一点，比如说我给个三万啊。这个东西为什么要给大一些哈？因为你不知道这个模型什么时候会训练完成，明白没有，我们是不知道这个模型什么时候会训练完成的。所以说我们一般在训练模型的这个呃轮次的定义就是你一共要训练这个轮次，这个什么意思？我解释一下啊。轮次是。训练完整的数据集的次数。啊，把这句话记住，

训练完整的我给你们写下来吧啊。轮次就是。完整。训练。呃将。整个数据集。整个。数据集。数据集训练完一次为一轮。啊，就这个意思啊，将整个数据集训练完一次为一轮，所以轮次呢，简单来讲就是我们训练完一整个数据集。所呃训练完这个数据集所需要的完整的次数明白了吧啊，

训练完这个数据集所训练的所需要的完整的次数啊，就这个意思。OK，好啊，那么这儿要给它定义大一些啊，我们这儿就给个三万次啊，为什么给这么大？因为我不知道它什么时候训练好。给大一些无所谓，因为中间我是可以手动停止的啊。好，那么下一步呢，就是增加我们这个talking，我们要编码吧，编码就把这个talking拿过来加载分词器啊。

这我就拷贝过来了。training里面把这个token给它加进来。啊，加载这个分词器啊，加载分词器。啊，叫它分词性好。那么，下一步就是我们的这个创建数据集。诶，创建数据集。training data set.就等于一个my dataset。这我们是训练集嘛。增加这个训练啊，

增加这个训练，然后来取数据，这个数据你得取啊，取的话我们构建一个变量叫做training loader。tree loader就等于我们的data loader。来取我们的数据啊，这个要取的数据集dataset给的就是我们刚刚这加的这个train data set。然后批次什么叫批次呢？就是每一次取几条数据来训练模型，这个批次的大小是个超参数，什么叫超参数呢？你得是。你得是因为我们一般情况下，大家都知道我们一直在讲啊，就是训练这个AI模型得有设备，

对吧？你比如说我今天晚上我拿这个gpu为例啊。我这个gpu 1共有六个GB的线存。什么情况下叫对这个gpu的训练利用率比较高呢？就是说一般情况下，我们在模型训练的时候，让它的这个显存保持到90%左右。啊，那就是对这个设备的利用率就比较高了。那么，这个90%左右你得试啊，你得试，因为模型大小不一样，这个批次，但你们记住一句话，

批次越大，显存的占用率越高，就这个意思明白了吧，批次越大，显存的占用率越高啊。只是我们今晚做增量微调的话呢，它那个数据量，它那个模型比较小，数据量不大啊，所以说它就很快啊，它就很快。呃CPU是看什么东西呢？CPU是看这个。看内存。啊，

内存就设计这个参数有只有一个标准。不要out of memory，如果出现了out of memory，你就把它往小调。理解没有啊，出现了out of memory，就把它往好往小调啊。哎，我看大家问的问的问题有点奇奇怪怪的，我我这个模型不是本地的吗？这不是本地的模型吗？我现在讲的不是就教你们怎么从本地模型上面做增量微调嘛啊？好，这是数据集啊，继续往下走。

然后还有个参数叫做suffer。这个设置为true什么意思呢？叫打乱数据集。啊，这个我这加个注释吧。训练批次。啊，就每一次给多少条数据？这叫打乱。啊，打乱数据集，防止这个数据它学到了不该学的这种序列信息啊。然后对于自然语义的这个数据集做加载处理的时候，我们还有一个参数叫做draw plus的，要设置为true。

这个东西叫做舍弃最后一个piece的数据啊。舍弃最后一个批次。的数据。防止。防止。形状出错。好防止行容出错，就是自然语里面都要加，这都要加这句话啊，都要加这句话，因为不加这个东西的话，你很有可能会出错啊。咳咳咳。我们解释一下，什么叫很有可能会出错，

是这个意思。比如你一共有100条数据。如果你的批次给的是。十那么它是不是一共加十次，每次加十条？能理解吧p次加十次，每次加十条，它每次都是十条，每次形容都是一致的，就不会报错，但如果你的这个数据集很恰巧是103条。你的批次给的是十，那意味着什么呢？它前面的几次是不是都是十对吧？但是最后最后一次的时候是不是就只剩三了？

你看这个三的长度是不够了，能理解吧，就会报错啊，就会报错，所以drop last的意思就是说最后一个批次数据。就给你舍去就不要了，防止形状出错啊，当然有同学在这会担心老师那舍去了，这个数据是是不是就不会被学到啊？不是的，为什么因为我这把数据打乱了？而且我这是学三万次。第一次学不到它，第二次就会学到它嘛，第二次学不到它，

第三次总会学到它的嘛，能理解我意思吧啊，所以说这个自然语义里面哈，因为我们处理的是个序列问题，你得保证序列的长度得一致。所以说你没有因为我们自己在这加批次的时候，不可能说是每次看一下这个批次是不是刚好能够被整个数据集的长度所整除，因为那样效率太低了。所以它专门提供了一个方法，可以把最后的那个部分给它舍去啊。好，那么接下来最重要的一个操作来了，我们前面做的这个data有一个毛病，在什么地方呢？注意看啊。

目前的这个data是不是还是文本？它是没法给到模型进行训练的。理解没有它，没法给它模型进行训练啊。我们要把这个文本编码成什么呢？编码成我们的这个input IDS啊，编码成input IDS，所以我们在这个里面做编码。对加载的。加载的数据。进行编码。进行编码，那么这编码的话呢？它提供了一个方法，叫做collect function。

collect collect function，这个collect function要求呃，它是允许用户可以自定义啊，可以对这个方法进行自定义的。那么我们就在这个dataset的前面哈。自定义一个函数。自定义一个函数就叫做collect function嘛啊，传入就是这个函数的意义是什么呢？将传进来的这个数据啊。将传递的数据进行编码。啊将传入的。传入的字符串。进行编码。啊，进行编码，

编码方法就是我们前面给大家讲的那个talking里面的东西啊，我们先把语句拿进来，先把它封装成是个语句啊sentence。就等于我们的嗯，这儿来遍历这个数据啊I0。注意看，因为这个数据是不是分为两个部分零？是不是是我们的这个文本，一是我们的标签啊，一是我们的标签。啊零啊否？for I in data.好，然后这是我们的语呃句子啊label。labels就等于。

I1。for I in data.啊，就是索引，一是label啊，零是我们的这个文本，先把这个文本和这个label拿到啊，那么下一步就是编码。呃，下一步就是编码。编码我们就把那个token里面的写给我那个部分拿过来就行了。呃，就这个部分拿过来就行。再拿过来，我们这得稍微改一改啊。

呃，这个返回我们就用data来表示啊，用data来表示呃方法跟刚刚讲的那个前面讲的那个token的base encode plus是一样的。这把你要编码的句子给它传进来，这个句子我就直接传我们这个sentence，这个这个句子就行了啊，sentence这个整个句子让它对于一整句话直接来进行编码。呃，这个参数可以不加，因为它默认就有啊，因为我前面给你们讲，所以说我把它给你加进来啊truncation，我们要留着啊，因为truncation它默认是没有的。默认是没用的，

然后接下来这里面有个参数叫max dense，这个max dense，你在实际做项目时候怎么来进行设计呢？是这样的，你得看一下你这个文本的，这个大概的这个长度是多长？啊，大概这个长度是多长？就是因为这个文本有个什么特点呢？你看啊，有些个别的数据，它会显得很长。嗯，看到没有你，比如说我，

我把镜头拉到这个位置啊。你其实拉到这个位置的时候，大部分的这个大部分这个样本诶，大部分样本其实已经包含进去了，再往拉到这个位置。你看啊，剩下的这个有有一少部分文本是在这啊，这个文本可能还有点短，我再往这拉起，起码你拉到这个位置的时候。你看到没有，这个样本就很少了，就它有极个别的样本是特别长的，能理解我意思吧，极个别的样本是特别长的啊，

那我我们在这怎么去设计这个长度呢？呃，其实看情况，我说一句话，看情况，如果你想要的效果是我尽可能的多去保留这个样本。那你就把这个max lens给到最大，我们给到最大是给可以给到512。啊，目前这个模型可以给到512，如果但是这个东西给大了的话呢，它对你的这个训练时候这个显存的占用率会高一些。啊，显存占用率会高一些啊，如果你希望对这个显存的占用率降低一点的话呢，

你就把这个呃长度可以适当的调短一些。但这个调短你得注意了，你不能说是打个比方，你比如我这个文本的平均长度是328。那我们就给它调到328就比较合适啊，你你不能太低，比如说你把这个人家平均长度是328，你把这个max lens你调成102。那会导致一个问题，模型输入的句子没有一句话是完整的，那么它在最后测试的时候，这个效果就不是特别好了，明白了吧啊，是这个意思啊？啊，

所以说根据你的这个设备的情况哈，因为我们现在这个模型本身的长度就不是特别长，512对吧？我们就给到512给到最大了啊，给到最大了。好。嗯，不是字节的长度，不是字节的长度，是字符数量。哎，你们前面没注意听哈，问这个同学的问题就没注意听哈，我最开始给你们讲过这个东西啊，这个是编码之后的这个。

字符的长度哈编码之后的字符的长度啊，得认真听啊，因为你稍一不注意，有些东西你就。掉队了啊。它最大长度是512，就是编码之后的这个字符长度啊，跟那个存储长度没关系啊，是字符的数量长度啊，编码之后的。好，最大我们就给到500h12，因为它本身不长啊，然后嗯max lens padding的话，一般都是max lens补齐。

呃，接下来我们是paddle是模型训练啊，这一次我们就不用long了，我们让它返回PT。注意，这是字符串啊PT啊，返回paddles的这个张量类型啊，返回paddles张量类型这三个参数可以不写，因为它默认都有。默认都有OK啊，return就可以了。好，那么这就编码就成功了啊，编码就搞搞完了，搞完之后的话呢，

我们得做一个处理，我们分别把。分别把这个模型输入所需要的三个参数给它拿进来。内那个net里面，我们刚刚不是给它设计三个参数嘛，对吧？第一个input IDS，我们分别从这给它取出来input IDS。就等于我们的data给它单独取啊，单独取input IDS这个玩意儿，我说一下。第一个小节里面东西非常重要，再给你们看一遍啊，这个东西out编码之后的返回，结果里面是不是包含了input id stalking type IDS，

special talking？mask还有这个lens对吧啊，我们得把它取出来，看到没有，这是人家返回的结果好，我们在这就把它单独取出来啊，input IDS。通过这个里里面给它检索出来。OK，然后是attention mask。attention mask.呃，就等于data。好给它检索出来attention mask，因为我们要单独返回啊，

单独返回还有talking type ideas。talking type id stalking type IDS也是给它单独取啊，单独取，单独返回啊，talking type IDS就等于我们的这个data。这儿单独做这个索引啊talking dids好，完了之后最后一个就是我们刚刚这儿加的这个label，我把这个label也给它返回啊libe啊label就等于我们这里面，我们对label做一个处理。因为我们前面那个label的话呢，它只是一个数字零，你在这里面要变成张量啊，因为我们这个paddles里面输入的所有的数据。所有的数据啊，都得转换成paddles的，

这个张量类型啊，转换成paddles张量类型啊，那么这里面就放一个。touch.点儿转换成long tensor啊，long tensor啊，转个数据类型就可以了。啊，把这个label传进来OK啊，那么这个所需要的这个三个输入和一个标签儿就处理完了，我们这儿再返回。return input IDS.呃attention mask，还有就是talking type IDS以及刚刚的这个label，这就搞定了啊，

搞定了。好，那么我们这个数据集这就处理完了，这是自定义的一个数据集的这个编码方法，编码完了之后这个函数呢，要把名称传到人家的这个collect function里面去啊。这切记，只传函数名，不传参。啊，这个函数里面只传函数名，不传参啊，传参就错了啊，不传参啊，不传参。

好，那么我们的数据加载器就搞定了，下一步呢，我们来写一个主函数啊，写一个主函数开始我们的这个模型的训练部分。好，第一步我们来开始训练。开始训练啊，开始训练。第一件事，先来print一下这个device，我们训练的时候先是打印这个设备信息啊。model.就等于模型，先把模型实例化。

把我们之前构建好那个模型实例化一样的，给它放到这个什么呢device上面去。放到device上面去啊OK，然后模型怎么来训练呢？第一件事是定义优化器。定义优化器。呃，定义优化器，优化器，优化器，我们刚刚这导了一个包啊，这个包的名叫做Adam w，我们一般现在在做大模型的这种微调训练的时候都建议用这个优化器啊。iw啊。mizer optimizer就等于我们的Adam w。

好，这里面去传入什么东西呢？传入我要微调的模型的参数。啊传入我要微调的模型的参数，下一步我们来定义损失函数。定义损失函数。呃，不要去管这个东西啊，我切完之后我再给你们讲这个东西，你怎么去学啊？先看就行了，待会我跟你说啊。loss function.损失函数loss function=touch点nn点，我们做的是个二分类问题啊，

二分类问题，我们二分类问题用的是多分类交叉商。啊，用东风的经销商OK好，那么有模型训练的优化器优化所需要用的loss函数都有了，那么下一步我们就可以开始训练了。啊，下一步我们就可以开始训练了，训练就是取数据开始训练for epoch。in range.就是这里面来定义我们所需要训练整体的轮次，轮次前面这定义了一个epoch，一共是我这写了个三万啊，让他训练很多次，对吧？

一直让他训练。好，然后我们在这个epoch里面去取数据，因为我前面讲过这个epoch是什么意思呢？它代表你把整个数据集训练完一次就是一轮。所以我们是在这个循环里面去取整个数据集去做训练的啊，好I把我们的数据取出来，好数据包含就刚刚这返回了这三个结果。把这把这个四个内容给大家拿过来，这是取出来的这个内容啊，取出来这个内容。呃label label label是什么？诶，我看一下。啊，

无所谓啊label啊label好把这四个东西取出来啊，把这四个东西取出来in。e merit.啊ime rate。从哪个里面来取呢？从我们的train loader里面来取，从train loader里面来取哈OK？好，接下来第一件事啊，因为我把模型是不是放到这个device上面去了，对吧？你的数据集得和模型放在相同的设备上面去啊，所以我们第一件事是将数据。放到device。device device上面。

将数据放到device上面去啊。好，这儿跟前面就一样的操作啊，跟前面一样操作就是把这四个值我们调用那个two device。啊，就等于后面我这加一下啊，挨个往后面加点to。点to。device.attention mask点to。device talking type ies点to device。啊，然后最后还有一个什么label点to device。啊，把每一个数据都放到这个device device上面去。

好，然后我们接下一步就是前项计算。呃，这这块东西其实跟你们它是算法的，这个开发逻辑啊，就给大家看一下这个框架好吧？前项计算啊诶。前项计算啊，前项计算什么叫前项计算？将数据。将数据。输入模型。得到输出啊，得到输出。out就等于我们的model点呃，

不是点啊，直接把参数传进去，就这四个参数。就是把模型里面这四个参数给它传进去，但是这注意一个注意一个细节哈。你看我前面在这定义网络的时候。我定义网络时候，我这个模型的输入的顺序是不是input IDS attention mask和talking time IDS对吧？所以说我在这train的时候我就可以。直接把这四个参数把这三个参数给它传进来，我就没有没有通过专有参数的形式给它传哈，但是你得保证顺序得是一致的。就是这儿的顺序跟这儿的顺序得是一致的啊，要么你就是专有参数给它传进去啊OK好，这只传三个啊，

只传三个input IDS attention mask和talking type IDS，这传到哪儿去了？传到模型里面去了，明白了吧？得到的就是out，得到的就是out好，这就是把数据输入模型，得到输出叫做前项计算。那么，下一步你得到输入之后，怎么训练模型呢？我们下一步是根据输出。根据输出计算损失啊，计算损失loss就等于我们的loss function。传入我们的out模型的输出和什么东西呢？

我们刚刚的这个label就根据模型输出的结果和我们的这个标签去计算两者的误差。啊，跟计算两者误差，然后下一步就是根据误差误差优化。优化我们的参数啊，根据误差来优化参数，好optimizer第一步点zero ground清空梯度。然后loss点backward。啊lost backward，这叫做自动更新参数。啊自动更新梯度自动去哪儿？啊，更新参数step。啊step好来了，这个东西这个东西是永远都不会变的啊，

因为算法开发里面它就必须得这样写啊。这就是梯度优化。梯度下降算法的优化方法啊，优化方法只不过我们这用的是im w这个优化器啊，来来优化的，它是一个改良之后的，这个梯度下降算法啊。这个东西对你们来说不重要啊，你只是把注释记住就行了，代码不需要去记啊，代码不需要去记。好，那么这就开始训练了，我们在训练过程中需要去观察训练的效果，我们是这样来做的，

就是每隔比如说我这里面每隔五个批次。每隔五个批次输出训练。信息啊，每隔五个配置输出虚拟信息if I取于于五=0。对吧，我们就输出一下这个序列信息啊，我们输出这么几个东西，一个是先把它的结果取出来。呃arg max dim=1。先把这个out的结果取出来，求一下计算训练精度。啊，计算一下这个训练的精度。scc就等于这里面，我们算一下精度啊out等于等于labels。

labels呃点sum。呃点sum点item。啊点item去除以这个什么呢length？labels啊，这是计算准确率啊。这个很简单，简单说一下。什么叫精度呢？你比如说。我们有五个数据，假如说他label啊。label是零零一零一对吧？然后你的这个out。如果你的alt是零一一零零，这个精度怎么算？

你看我们这算法啊。这个我先问一下你们这个准确度是多少？它算对了几个，一个两个三个是不是五分之三？五分之三是不是就它的一个精度对吧？那这个精度我们这怎么来算呢？我判等一下嘛，判等一下嘛。你的out等于等于label等于等于AA label，这个out判等于label怎么来判断呢？判断结果是个布尔值为true或者FALSE。这个应该你们都知道啊，判断为true或者FALSE那么true，其实就是一个非零值FALSE就是0 true是一。啊，

初始一啊，所以我直接一判，等一一一求和这个东西一判，等一求和你看一下啊，这个是不是？一个对吧零？一一零一求和是不是就是三再除以整个内部的长度是不是就五分之三？看到没有？这这是一个精度的一个计算方法啊。好，这个准确度就是我们看一下这个模型的，这个训练过程中啊，它的精度有没有在上涨？如果精度逐渐的往上涨，就意味着它的这个。

这个训练的效果是越来越好的啊。好，然后我们在这来。呃，输出一下这个信息啊，对预测值和真实值的比较啊，预测的信息epoch。epoch啊，输出一下这个轮次epoch。然后是它的这个批次。呃，这个批次I。完了，这是los。呃，

我们把这几个参数都做一个输出los。lost点iter。lost点item。好，最后是我们刚刚写的这个精度啊。呃，我们刚刚写的这个精度啊ACC。好这儿就输出这些信息啊，就是训练的第几轮第几个批次损失损失正常情况下应该是在往下降的。loss往下降，精度应该是往上涨的哈。好，然后我们接下来要做一个操作，就是你训练的结果得做一个保存啊，我们后面要用嘛，

对吧？我们保存是这样来保存，一般是每一个轮次做一个保存。所以说是在这个for循环。是在这个一轮一个轮次之后啊，注意看清楚是在这个呃epoch的循环内部。啊，这个数据的循环到外面啊，数据循环到外面好。每训练每。训练完一轮。每训练完一轮，保存一次参数啊。每训练完一轮，保存一次，

这个参数怎么来保存呢？touch。rch touch.点safe。保存啊，我们保存模型的参数。state的act。给它保存到哪里去呢？给一个路径。给一个路径。呃，paragraphs下面我们定义一个这个呃epoch。就是每一轮我都保存啊，每一轮我都保存，所以说名称是以boke开头的bert。

点pd h啊，一般我们这个权重paddles就是pd h的这个权重好，然后做一个输出就完事了啊epoch。呃，给个提示参数保存成功。OK，这就搞定了。哎，这就搞定了。好，那么这个模型的这个训练部分我们就写完了啊，写完了接下来我们先来跑一跑啊，跑一跑。我这边p4给的是100不大啊，你们每个人拿去都可以训练。

每个人拿去都可以训练，我们这训练给你们看一下啊。但是这个方法要注意一下，就是它这个s if里面啊，如果你给这个文件夹的话呢？嗯，这个文件夹得手动创建，因为paddle这个save方法，它好像是不创建文件夹的。啊，我记得它好像是不创建文件夹的。所以如果你没有这个目录的话，它可能会报错。好看到没有，注意看啊。

注意看，注意看啊，我就给你们说这个错误。你看啊，这儿是不是我一跑报了一个错误，叫out of memory叫could out of memory什么意思呢？p次给的太大了，它装不下。明白了吗？p4给的太大了，它装不下啊，那么如果各位遇到这种情况怎么办？你把它调小一些。调小一些哈，我这儿适当适当性的给个十，

我们先试，你这试怎么试？你得打开你的这个，比如说你用的cud a，你得看你的这个。显存显存的占用率大概是多少？我们让它控制在90%就可以啊，控制在90%就可以。你看我p4给到十，它跑到这个位置。好，那给到十跑到这个位置的话呢，盲猜一下，它应该是可以给到。30，

40应该没问题啊，这种的话就有点低了啊，你为了让它让你这个设备的利用率上来啊，你就。稍微调大一点，我这个给个40应该差不多哈，所以这为什么叫超级参数就是你得试哈，你得自己测不同的设备呢，这个值给的给的不一样。懂了吗？不同的设备，这个值给的不一样哈，如果你们CPU的话呢，你看内存啊，如果是CPU的话，

就看内存啊。嗯，41半。40是一半啊。那按道理刚刚100不会炸呀，可能是我开了这个什么直播的原因哈，那这个利润率还不够，还得往上调啊，还得往上调。给个给个90嘛，给个90应该是可以的。呃，所以这个参数就是试出来的啊，这个参数就是这样，就是试出来的啊AI训练上面这个参数就是就是试出来的，

包括你们以后自己去微调大模型一样的。你自己为啥弹幕群上面我们得给这个超三得给这个p4这个超三？哎呀，这个拍摄目有时候跑多了就卡了。咳咳。嘿，我怎么还把它断不了了？哎，我勒个去。啊关了，重开一下。哎呦，这个拍照嘛，真的是很麻烦啊。好关了，

重开一下。呃，我们现在来看一下啊，我刚刚这给的是八给的九十九十应该是可以跑的。它那个什么东西啊？它那个进程没杀死。它刚刚那个进程没杀死。我们来关网吧。关完重开一下，关了之后。呃，这个拍上面进程没杀死，因为你看它显存还在占。然后后台跑了个进程。后来一个p进程给它结束了。

呃，这个p现在新版本的这个pattern，我就发现它不是那么的好用啊嗯，你们要注意注意一下啊。它有时候那个p的那个进程，它有时候关不住，关不住，关不住，你把它手动给它终止了，重开一下就好了啊。咦，这个要特别注意一下啊，这就是属于细节性的问题了啊。尤其是我们后面训练大模型一样的道理。那些很大的模型啊，

我的意思是动不动就是十几个GB，几十个GB那种东西啊，你得观察你的这个显存或者内存的一个占用情况哈，来判断它到底有没有被释放掉。好嗯。啊，这样吧啊，我不在p里面训练这个，这个p里面训练有点卡啊。因为我这边开的东西有点多。嗯，我把它。用那个cmd来给你们跑一下。你们看一下这个效果啊，看这个训练效果。

啊，我们可以这样跑，这样跑就比较简单啊。直接去执行这个training啊。好一样的啊，这来看一下。你看我刚刚这p4给的是90，90应该是可以跑的，我六个GB上面那个90应该是没问题的啊。好，这儿看设备是哭的，看到没有啊？设备是哭的。哎，过道涨起来了，

就是我之前设的100应该是可以跑的，刚刚100不能跑，应该是有一部分的，这个什么呢？显存啊，被直播或者录播给占用了啊，就是我顺带给你们，它出错了，倒也好给你们演示一下它那个超算怎么设计的，明白了吧？就是这个东西，大家得根据这个参数啊，这个参数你们得根据你们自己设备的情况来进行调整，就是大了的话呢，你应该调小小的话调大啊，

那么我们判断大小的依据是这个显存的占用率基本到百分之。90左右就可以了，明白了吧啊，显存的占用率基本到90%左右就行啊。好看到没有，这已经开始训练了啊，开始训练了。那么，你怎么判断一个模型有没有正常训练呢？就是我们得看它训练时候的这个loss。这个loss啊，我把它放大一点点。这个loss它应该是逐渐的降低精度，应该是逐渐的升高的。啊，

精度应该是逐渐的升高的啊。呃，但是要多观察几次啊，至少要跑到这输出个七八次八九次的时候呃，我说一下啊，它不是看每一次的结果，我们看的是什么？看的是趋势。看的是趋势啊。记住，看的是趋势。明白了吧，你看这个loss呈下降趋势，精度呈上升趋势，哎，

那就证明你的当前的这个训练是OK的。你看我们目前这输出这三个结果，这三个结果是不是都呈lost都呈下降趋势的，对吧？精度都呈上升趋势的。啊，那就证明它这个训练的方向是正确的哈，你让它训练就行了好吧啊，你让它训练就行了呃。但是我这边的话呢？我让它跑完一轮，因为这边有个错误，我让你们报，我要给你们说一下啊，就这个函数啊，

这个torch点save这个函数。我们如果说在前面加了一个文件夹路径，它会报错啊，我记得之前的版本会报错新版，我不知道它有没有修复，就这个文件，它没法自己创建。我们得手动创建，或者说我们得通过这个别的代码来给它创建哈，让它跑嘛，跑完我看下我这边是9000条数据。9000条数据给的是100，那就得跑90次。四九十次哎呦，这个90次时间还有点久了。

哎，94时间有点儿久了呢，我们这个时间还不够啊，今晚有点儿超时了，时间还不够呃，我就直接把这个问题给你们说了啊，你到时候注意一下。注意一下，因为这儿保存的时候，这行代码会报错，它会提示你这个路径不存在，所以我们最好是提前手动把这个文件夹创建好啊。当然，你们尽可能的让他报一个错啊，报个错对你们来说是有好处的，

好吧，报个错对你们来说是有好处的哈，你知道这个东西怎么解决就行了啊？OK，那么这是我们的这个嗯。嗯，情绪中文情价呃，这个评价的情绪识别的这个模型啊，完整的一个。从数据到模型的设计啊，到模型的这个训练，这个过程，当然我们今晚看不到结果啊，就是大家下来之后，你们先去训练。

训练完了之后，我们下节课教你们怎么去看这个结果，我说下这个东西，你训练到什么时候可以停下来啊？训练到什么时候可以停下来，就是你尽可能的让他的这个损失变得比较低，你记住啊，你现阶段现阶段你们不要去管其他的，就让这个loss比较低。让那个精度比较高，这个精度最高应该是可以接近，可以到一上面去啊，这个精度最终可以到一精度，精度到一就可以停下来了。这个准确率到一的时候就可以停下来了啊，

就可以停下来了。好啊，这个到时候大家自己训练一下呃，我们完了之后的话呢，这个par par paragraphs下面会保存一个。pd h的一个文件。明白了吗啊，保存一个pd h的一个文件啊，好，我这边就不用它来训练了哈啊，就不用它来训练，这个就是我们今晚完了之后留的一个作业跟任务啊，你们到时候自己做一下。呃，这我说说几个重点就是关于这篇代码，

前面有同学有问题，我要简单做一做个介绍啊。嗯，我说过了，前面的这几节课是最难的，到后面的这个实操会越来越简单，为什么会越来越简单？注意听。注意听啊。我们后面用的大模型，比如说像千万啊，还有lamer lamer模型，还有像这个deeps这些模型哈。它都是跟这个我们今晚用的这个bert是一样的。听清楚，

都跟这个b是一样的结构，什么东西都是一样的，唯一的区别在于它更大。它中间对于某些模块做了优化的，但是对于我们应用的流程来讲是一模一样的，大家以后做做任何的这个模型微调的开发跟我们今晚的这个过过程也是完全一致的。能明白我意思吧，也是完全一致的，我们都是从数据模型，训练评估部署。整个流程开始的，但是不一样的地方在哪呢？不一样的地方在于我们今晚的整个流程是完全自己从零开始写代码实现的。听清楚啊，我们今晚的这个过程是从零开始自己写代码实现的，

从比如说从这个数据的获取。看到没有，从数据集的制作，然后从模型的这个增量模型模型的这个设计。啊，还有就是这个模型的，这个训练的过程是不是我们全部写代码？写写代码做的，我说一下这个代码不要求各位掌握，为什么不要求各位掌握呢？因为以后啊。以后就是我们这个第一个阶段结束之后呢。我就会教大家用。框架来微调，我们真正的这个大模型。

那么那个时候呢，你的这个数据。就是这个数据集的这个制作。这个模型的增量的这个设计，还有就是这个模型训练的这个代码都不需要我们自己来写了。框架把我们做好了。理解没有，我们只需要去使用那个框架进行参数配置就行了，那我为什么要讲这个部分呢？因为这个东西是我们自己完全自主从零开始设计出来的东西。它能够我希望它能够帮助各位更好的去理解一下。我们原理性的东西明白了吧，我们做自然语言的应用开发，我觉得最起码你得懂文本怎么编码？AI的这个开发流程是怎么样的啊？

大家知道，就是它的内部在做哪些事就行了。懂了没有？当然有基础的同学，你可以去掌握这个东西啊，那掌握了最好啊，掌握了最好。好，我觉得最重要的是我加的这些注释就是大家要知道它框架，我们后面调框架的时候，它里面在干哪些事？你把这个东西搞明白就行了啊，还有一点就是说嗯，这个代码呢，它是有很强的实用价值的，

因为。我们现在写代码就只有这么几个文件，但是你们以后如果用框架的话呢？这个框架会显得很。很笨拙，而且后面的框架，它其实严格意义上来说，它是没法做这个分类任务的，我们目前的大模型框架都是做生成任务的。所以这个代码的话呢，到后面是有很强的扩展性的啊，听清楚是有很强的扩展性的，只是说今天的话呢，我们才开始做训练啊。下节课我们会基于就是大家在这边训练的这个群众来把它做一个完整的这个呃测试的一个流程。

你们到时候你们就会能够理解到这个东西的价值在哪了，并且它的扩展性很强，因为我们是完全自主实验的一个代码。你看我们现在做的是二分类任务，对吧？如果我们后面要做八分类任务，要做十分类任务，怎么来做啊？这个我们到下节课给大家做一个这个讲解和介绍啊。好，那么今天我们就把这个任务先做到训练的部分，大家下去之后的任务呢就是。根据你们不用自己去写代码，你根据我提供的这个代码。去训练一套自己的这个权重出来好吧啊，

去训练一套自己的权重出来那个精度啊，训练到一。啊，训练到一，然后那个loss呢？越低越好，当然我们这有个最低标准哈，最起码你得训练完一轮啊，最起码得训练完一轮，因为只有一轮之后这才有参数啊，有了参数之后我们才可以用来做测试。一直没有。呃，英文一样的啊，英文一样的，

我们到后面其实英文这个东西编码跟中文编码是一样的。它的不同的区别在于，它用的字典不一样。能懂我意思吧，它用的字典不一样。啊，它用的字典不一样。你们如果想看，我给你教一个方法。啊，你我给你，你们想看我们教一个方法，我这下的模型是不是叫bert？我这是不是叫bert base Chinese是吧？对不对？

你下一个英文的模型不就完事了吗？能懂我意思吧你哈根费斯上面你下一个bert模型不就行了吗？它不带Chinese的。它不带Chinese的啊，不带Chinese的，那就是非中文的，你可以把它下下来看一下它那个vocabul。你就懂了，跟这是一样的。只是说它对中文的分词不同，我们中文现在的这个分词，它是单个的字，英文人家是一个word啊。它是以word区分的呃，英文是word，

英文分词是word呃，就是一个个的这个英文单词，它不是字母啊，它不是字母，它是单词。中文是单个的汉字。好，行吧，我们今晚的内容就先到这里啊。
